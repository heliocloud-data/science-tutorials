{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595474ac-751d-4165-ba79-a7abdcb7caf7",
   "metadata": {},
   "source": [
    "# Guide to HelioCloud Tutorials\n",
    "S. Antunes (APL)\n",
    "July 2025\n",
    "\n",
    "HelioCloud is a cloud platform, an analysis cache of datasets, and an exploratory platform with tutorials to get researchers started. Our best introduction to 'why' is our [AGU_Demo-Populated.ipynb](AGU_Demo-Populated.ipynb) Notebook, which explains what HelioCloud can do and introduces a basic Python science example for looking at time-series data and at images.\n",
    "\n",
    "There are many tutorials here and we'll help walk you through them. At this point we'll assume you're already in your account and able to run a Notebook.  We will walk through examples of data reads from AWS S3 storage, using Dask for compute power, using the core PyHC packages in Python, and working in IDL.  We also include a link to a local copy of the PyHC summer school package tutorials for SunPy, SpacePy, AstroPy, HAPI, and others.\n",
    "\n",
    "The core HelioCloud notebooks to date are:\n",
    "1) Overall intro in [AGU_Demo-Populated.ipynb](AGU_Demo-Populated.ipynb)\n",
    "2) basic file access of FITS, CDF and NetCDF data that is stored in AWS S3 cloud storage, in [S3_Explained.ipynb](S3_Explained.ipynb)\n",
    "3) 'bursting' a job onto multiple temporary CPUs in [Dask+S3_Demo.ipynb](Dask+S3_Demo.ipynb)\n",
    "4) finding datasets and lists of data files within one more more HelioClouds, in the [CloudCatalog_Demo.ipynb](CloudCatalog_Demo.ipynb)\n",
    "5) extended example SDO: searching for SDO data then processing a large set on multiple CPUs via Dask and gathering the results, in [SDO_Demo.ipynb](SDO_Demo.ipynb)\n",
    "6) a brief 'everything' tutorial including how to do all the above in brief, and a walkthrough of each core PyHC package, in [Testing_Notebook.ipynb](Testing_Notebook.ipynb)\n",
    "\n",
    "We start off with the 'Science in the Browser' approach where the Juptyer Notebook suffices to find, analyze and plot data entirely within the cloud.  We also include additional material for power users who prefer to work in their own cloud VM or cloud console environment.\n",
    "\n",
    "## About HelioCloud, Notebooks, and Dask\n",
    "\n",
    "HelioCloud is a software stack enabling you to do research in the Amazon Web Services (AWS) high performance cloud from within your browser or laptop terminal.\n",
    "\n",
    "\"Daskhub\" is the name of the web-based Jupyter Notebook interface for most HelioCloud users, which enables running these notebooks.  'Dask' itself is a way to quickly parallelize your code by throwing it to multiple CPUs.  So Daskhub is a Notebook-based IDE that has access to dask 'burst' processing.\n",
    "\n",
    "\"Jupyter Notebooks\" aka Notebooks are a way to write code and descriptive text in the same document.  Notebooks also store output, and can serve as a publication of both code, documentation and results.  The intent is that a Notebook that works in one HelioCloud will work in other HelioClouds.\n",
    "\n",
    "If you are interested in how to write in Jupyter notebooks to make attractive presentation-ready pages, read the [Additional/OutputTypes notebook](Additional/OutputTypes.ipynb)\n",
    "\n",
    "\n",
    "### First: What is S3?\n",
    "\n",
    "S3 stands for \"Simple Storage Service,\" which provides object storage for for AWS. https://aws.amazon.com/s3/\n",
    "\n",
    "It allows people to query and access data from a common location reference. The buckets can be made web accessible to users outside of daskhub if web access is enabled.\n",
    "\n",
    "S3 buckets are individual storage elements.\n",
    "\n",
    "## Everything at Once\n",
    "\n",
    "If you are new to everything, look at our short [AGU_Demo-Populated.ipynb](AGU_Demo-Populated.ipynb) tutorial.\n",
    "\n",
    "If you are already familiar with Python and PyHC, the  brief 'everything' tutorial in [Testing_Notebook.ipynb](Testing_Notebook.ipynb) will show you how to (briefly) do typical desktop science tasks, but in the cloud.\n",
    "\n",
    "## Science Part 1: Cloud storage and using multiple CPUs in Python\n",
    "\n",
    "Python practice examples for reading sample data in FITS, CDF or NetCDF that are stored in this cloud is in our [S3-Explained notebook](S3-Explained.ipynb). These make use of AstroPy for FITS files, cdflib for CDF files, and Xarray for NetCDF files.\n",
    "\n",
    "Dask is software that lets you 'burst' a job onto multiple temporary CPUs by defining then using a cluster of CPUs to lazily parallelize jobs. Using Dask in a notebook is in [Dask+S3-Demo.ipynb](Dask+S3-Demo.ipynb), showing both generic Dask usage and an actual SDO calculation example case.\n",
    "\n",
    "## Science Part 2: Big Data Sets\n",
    "\n",
    "Cloud means never having to download datasets. Instead, you find data across multiple HelioClouds and directly access it without downloading it locally.  The [CloudCatalog API](https://pypi.org/project/cloudcatalog/) on top of the CloudCatalog sharing standard enables finding and listing cloud-stored scientific datasets such as CDAWeb, SDO, MMS and others.\n",
    "\n",
    "Our initial example for finding datasets and lists of data files within one more more HelioClouds is in the [CloudCatalog_Demo.ipynb](CloudCatalog_Demo.ipynb), using the MMS dataset.\n",
    "\n",
    "We then add a big data task using Dask 'burst' capabilities to tackle 2TB of data rapidly.  Here we search for SDO data then processing a large set on multiple CPUs via Dask and gathering the results, in the [SDO_Demo notebook](SDO_Demo.ipynb)\n",
    "\n",
    "## Working with IDL\n",
    "\n",
    "We provide an example for using IDL in the [IDL/IDL_examples notebook](IDL/IDL_examples.ipynb) and, additionally, accessing S3 in IDL in the [IDL/IDL-S3 notebook](IDL/IDL-S3.ipynb)\n",
    "\n",
    "## Power users\n",
    "\n",
    "Pushing data on or off S3 using SFTP is in [Setup/SFTP service notebook](Setup/SFTP service.ipynb)\n",
    "\n",
    "A simple \"hello world\" in Fortran is in the [Additional/fortran_helloworld notebook](Additional/fortran_helloworld.ipynb)\n",
    "\n",
    "An example of plotting a contributed dataset is shown in [Additional/EUVML_viewer](Additional/EUVML_viewer.ipynb)\n",
    "\n",
    "Updating your personal Conda environment is in the [Setup/Conda_instructions_for_cloud notebook](Setup/Conda_instructions_for_cloud.ipynb)\n",
    "\n",
    "A helper function for saving Portal-generated AWS keys into your local ~/.aws/credentials directory is in [Tools/Save_Credentials](Tools/Save_Credentials.ipynb)\n",
    "\n",
    "\n",
    "Testing if GPUs are enabled, in the[Additional/GPU-Info notebook](Additonal/GPU-Info.ipynb)\n",
    "\n",
    "# Sample PyHC Tutorials\n",
    "\n",
    "To get a feel for using a Notebook (whether in cloud or on your laptop), we provide an excerpt of tutorials from the PyHC 2022 Summer School.  This highlights using the standard packages, fetching data, and plotting where the cloud usage and laptop usage are identical.\n",
    "\n",
    "1) [AstroPy_FITS-files-demo.ipynb](pyhc/AstroPy_FITS-files-demo.ipynb)\n",
    "2) [HAPI_01.ipynb](pyhc/HAPI_01.ipynb) basics and\n",
    "[HAPI_03.ipynb](pyhc/HAPI_03.ipynb) plotting\n",
    "3) [Kamodo_04-Visualization.ipynb](pyhc/Kamodo_04-Visualization.ipynb)\n",
    "4) [PlasmaPy-tutorial-instructor.ipynb](pyhc/PlasmaPy-tutorial-instructor.ipynb)\n",
    "5) [PySPEDAS_Summer_School_2022.ipynb](pyhc/PySPEDAS_Summer_School_2022.ipynb)\n",
    "6) [SolarMach_notebook.ipynb](pyhc/SolarMach_notebook.ipynb)\n",
    "7) [SunPy_part-1-search-and-download_Instructor.ipynb](pyhc/SunPy_part-1-search-and-download_Instructor.ipynb), \n",
    "[SunPy_part-2-data-structures_Instructor.ipynb](pyhc/SunPy_part-2-data-structures_Instructor.ipynb) and\n",
    "[SunPy_part-3-coordinates_Instructor.ipynb](pyhc/SunPy_part-3-coordinates_Instructor.ipynb)\n",
    "\n",
    "## About HelioCloud\n",
    "\n",
    "HelioCloud is cloud software for the Heliophysics research community.  HelioCloud is a time-saving tool for heliophysics researchers to rapidly access and analyze high-volume datasets from a web browser.  It includes easy-to-navigate cloud-based software with big data storage offers an innovative, streamlined approach for conducting research.  An Open Science framework breaks down barriers to collaboration by enabling multipoint access to shared data, code, and analysis tools in a secure environment.  You can download and install the software at your institution to connect with other HelioCloud communities and contribute to the project.  More at [https://heliocloud.org](https://heliocloud.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c05a0-7a87-4c1d-933b-87515dddfbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
