{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edac1c14-2f71-4434-a3d3-ef4cfefddd6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HelioCloud at AGU 2024\n",
    "## Big data, burst capability, collaboration, science-in-the-browser\n",
    "\n",
    "### This notebook showcases HelioCloud, the NASA-created cloud platform for big data, high end computing, and data collaboration that any institution can set up.  It includes Petabytes of CDAWeb, MMS, SDO and contributed datasets.  It features:\n",
    "\n",
    "1) Science in your browser with a Python environment that 'just works'\n",
    "2) Ability to search and find data from many missions\n",
    "3) Computing on datasets without having to transfer them to your laptop\n",
    "4) Analysis and plotting interactively in Jupyter Notebooks\n",
    "5) Spinning up 100+ temporary CPUs or GPUs for a big analysis task using Dask\n",
    "6) Examples from many of the PyHC core packages, such as HAPI, SunPy, Kamodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d42eff-17e3-4bfd-8116-c6f91c93b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we have the proper versions, and plotting capability\n",
    "import os\n",
    "import pkg_resources\n",
    "try:\n",
    "    pkg_resources.require(\"cloudcatalog>0.5.0\")  # modified to use specific numpy\n",
    "except:\n",
    "    %pip install cloudcatalog --upgrade\n",
    "    os._exit(00)\n",
    "import matplotlib.pyplot as plt\n",
    "# optional, prettier inline Notebook plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg'] \n",
    "import warnings\n",
    "warnings.filterwarnings( \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afebbf7-65cc-4aa7-b0a4-be911f5e43c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HelioCloud shared cloud file registry (cloudcatalog)\n",
    "\n",
    "### This is a simple standard for any dataset that enables users to access it via an API or directly.  You ask for a dataID over a time range, and it returns pointers to the relevant files.\n",
    "\n",
    "The short definition is:\n",
    "\n",
    "    * Anyone can publish an S3 disk with a 'catalog.json' describing their datasets\n",
    "    * Each dataset has a flat-file <dataid>_YYYY.csv index of its contents\n",
    "    * These fetchable indexes have the form \"start, stop, file_location, filesize\" (plus optional metadata)\n",
    "    * A python client allows easy search and fetch using data IDs and time ranges\n",
    "    \n",
    "### Here is an example querying for the filelists for MMS Survey data and SDO's AIA images for all of Aug 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abaf60-592b-44a1-88ad-49785c05492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudcatalog\n",
    "mms_ids = cloudcatalog.EntireCatalogSearch().search_by_id('srvy_ion')\n",
    "aia_ids = cloudcatalog.EntireCatalogSearch().search_by_keywords(['AIA 0193'])\n",
    "print(f\"ID search: found {len(mms_ids)} IDs matching 'srvy_ion' and {len(aia_ids)} IDs matching 'AIA 0193', picking first of each set.\")\n",
    "\n",
    "mmsID, aiaID = mms_ids[0]['id'], aia_ids[0]['id']\n",
    "start, stop = '2020-08-01T00:00:00Z', '2020-08-30T00:00:00Z'\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "mms_files = fr.request_cloud_catalog(mmsID, start_date=start, stop_date=stop)\n",
    "aia_files = fr.request_cloud_catalog(aiaID, start_date=start, stop_date=stop)\n",
    "print(f\"Filelists: found {len(mms_files)} {mmsID} files, {len(aia_files)} {aiaID} files for time range {start} to {stop}\")\n",
    "file1 = mms_files.iloc[0]['datakey']\n",
    "print(f\"\\nFor example, here is our {mmsID} files\")\n",
    "mms_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228e434-b251-483e-9ea3-a8a1bb0cb184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's do something useful: plot an MMS time series, plot some SDO images\n",
    "\n",
    "\n",
    "Here we go. We will plot an MMS's FEEPS electron survey files and a sequence of SDO's AIA 193A EUV images. For MMS can use the 'cdflib' to remotely read the cloud-stored MMS datafile and plot it as usual.  This is identical to working with CDFs on your laptop, but with no network transfers. The data and the computation occur in the AWS cloud.\n",
    "\n",
    "### We first plot several MMS magnetic field and proton count values. Then we will generate then plot AIA EUV images (using AstroPy).  Again, we are not copying the disk to our machine, but reading directly from the cloud to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b127a5-0341-44ef-a646-e7879068b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdflib\n",
    "from matplotlib import pyplot as plt\n",
    "with cdflib.CDF(file1) as fin:\n",
    "    varnames = fin.cdf_info()['zVariables']\n",
    "    x = fin.varget(\"epoch\")\n",
    "    y = fin.varget(varnames[5])\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(varnames[5])\n",
    "    plt.title(f\"Plot of {varnames[5]}\")\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30120d3-0d25-481b-832f-2815cefe5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import sunpy.visualization.colormaps as cm\n",
    "import numpy as np\n",
    "\n",
    "cmap = matplotlib.colormaps['sdoaia193']\n",
    "for i in range(0,2200,700): #ele in aia_files.iloc[1:4]: #np.nditer(aia_files, REFS_OK=True):\n",
    "    fname = aia_files.iloc[i]['datakey']\n",
    "    try:\n",
    "        hdul = astropy.io.fits.open(fname)\n",
    "    except:\n",
    "        import s3fs\n",
    "        fs=s3fs.S3FileSystem(anon=True)\n",
    "        hdul = astropy.io.fits.open(fs.open(fname))\n",
    "    if i > 0:\n",
    "        plt.imshow(np.log(hdul[1].data), cmap=cmap)\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42612e7-9056-4cfe-b4d5-6651e3e5ad1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We can also use PyHC packages in this browser.\n",
    "\n",
    "### We've already shown AstroPy, and now we give examples for HAPI time series data, SunPy image data, and Kamodo model generation.\n",
    "\n",
    "These HAPI time series fetches or SunPy image calls or Kamodo model generations, executed within this Notebook, are the same as it would be on your laptop.  \n",
    "\n",
    "For HAPI, we look at DST1800 and Proton_QI1800 from OMNIWeb, from the 2022 PyHC Summer School tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dce13-2246-4686-9d1c-9d8a04593f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hapiclient import hapi\n",
    "from hapiplot import hapiplot\n",
    "###    import math\n",
    "# HAPI test, OMNIWeb data\n",
    "# The data server\n",
    "server     = 'https://cdaweb.gsfc.nasa.gov/hapi'\n",
    "# The data set\n",
    "dataset    = 'OMNI2_H0_MRG1HR'\n",
    "# Start and stop times\n",
    "start      = '2021-10-25T00:00:00Z'\n",
    "stop       = '2021-12-01T00:00:00Z'\n",
    "# The HAPI convention is that parameters is a comma-separated list. Here we request two parameters.\n",
    "parameters = 'DST1800,Proton_QI1800'\n",
    "# Configuration options for the hapi function.\n",
    "opts = {'logging': False, 'usecache': True, 'cachedir': './hapicache' }\n",
    "# Get parameter data. See section 5 for for information on getting available datasets and parameters\n",
    "data, meta = hapi(server, dataset, parameters, start, stop, **opts)\n",
    "meta = hapiplot(data, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05032d-1c1f-4ba8-ad28-572da28f5c65",
   "metadata": {},
   "source": [
    "### SunPy\n",
    "\n",
    "### This is the SunPy AIA Demo\n",
    "\n",
    "We fetch a single demo AIA 171A EUV image and adding in WCS and prettier plotting. This is from https://docs.sunpy.org/en/stable/tutorial/maps.html. Again the code to run this is identical to your laptop code, because HelioCloud is about science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101decc3-3680-49b2-abe8-cb63db2d08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sunpy.map\n",
    "import sunpy.data.sample\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "sunpy.data.sample.AIA_171_IMAGE\n",
    "my_map = sunpy.map.Map(sunpy.data.sample.AIA_171_IMAGE)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=my_map)\n",
    "my_map.plot(axes=ax, clip_interval=(1, 99.5)*u.percent)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b2127-5803-4f0b-a4ed-7ea883bbe33b",
   "metadata": {},
   "source": [
    "### Kamodo modeling\n",
    "\n",
    "Here we run the Kamodo model generator example from the 2022 PyHC Summer School.  \n",
    "\n",
    "### Note the use of interactive plots-- you can use the mouse to rotate or change the plot size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576d790-44f2-4324-87b3-51e2a8ae84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamodo import Kamodo\n",
    "import numpy as np\n",
    "x, y, z = np.meshgrid(np.linspace(-2,2,4),\n",
    "                      np.linspace(-3,3,6),\n",
    "                      np.linspace(-5,5,10))\n",
    "points = np.array(list(zip(x.ravel(), y.ravel(), z.ravel())))\n",
    "def fvec_Ncomma3(rvec_Ncomma3 = points):\n",
    "    return rvec_Ncomma3\n",
    "k = Kamodo(fvec_Ncomma3 = fvec_Ncomma3)\n",
    "k.plot('fvec_Ncomma3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606e026-3390-43d3-a2d0-8bcdcddf4159",
   "metadata": {},
   "source": [
    "## Ability to run a task with Lambdas (lambdas test) \n",
    "\n",
    "### Python Lambdas are a quick way to run a function on a large set of data with just 1 line of code.  \n",
    "\n",
    "Here we do a very simple checksum on MMS files fetched from a CloudCatalog query.  You could just as easily add a function that does a more meaningful calculation, then run it on all the data with one line of invoking code.\n",
    "\n",
    "Again, with HelioCloud we are skipping the usual sluggish copying over of data before processing, and instead doing the computation on the data directly in the cloud.  This provides you with ready access to data, avoids filling disks, is faster, and lets you share code so others can collaborate (they'll already have access to the Petabytes of data in HelioCloud's public cloud archive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19079fc8-9c3e-4166-9c38-13d24bd46f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudcatalog\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "dataset_id1 = 'mms1_feeps_brst_electron'\n",
    "start = '2020-08-01T00:00:00Z'\n",
    "stop =   '2020-08-30T00:00:01Z'\n",
    "file_registry1 = fr.request_cloud_catalog(dataset_id1, start_date=start, stop_date=stop, overwrite=False)\n",
    "print(f\"{len(file_registry1)} files, operating lambda on first ten.\")\n",
    "print('Python Hash of File | Start Date | File Size')\n",
    "\n",
    "fr.stream(file_registry1[0:10], lambda bo, d, e, f: print(hash(bo.read()), d.replace(' ', 'T')+'Z', e, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9feddd-ff01-4b93-a8b8-3029f8cd1b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloud 'throw lots of CPUs at a problem' (Dask cluster) burst test\n",
    "\n",
    "A big advantage of HelioCloud is the ability to send a computation to multiple CPUs, then gather the results.  This uses a library called Dask, which looks similar to a Python Lambda.  For example, given a function that does science 'process_fits_s3' and  a list of files 's3_files', the code is simply:\n",
    "```\n",
    "time_irrad = client.map(process_fits_s3, s3_files)\n",
    "all_data = client.gather(time_irrad)\n",
    "```\n",
    "There is an initial time hit to spin up the Cluster of extra CPUs, but then this and subsequent jobs are very rapid. Our code block 1 sets up the data environment and has our 'DO_SCIENCE' analysis routines. Block 2 starts the cluster.  Block 3 does the dask run (and can be re-run).  Once done, Block 4 shuts down the cluster for good.  Again the cloud data is in AWS S3 and we do not copy the files over, but instead have CPUs at AWS access it directly.  \n",
    "\n",
    "### 1 year of SDO 94A EUV images from AIA is 129,758 files, each 14MB, totalling 1.8 TB.  If done serially on your laptop, it would take 27 hours.   HelioCloud takes 25 minutes (1467 sec) to analyze through 1 year of SDO data.\n",
    "\n",
    "(For demo purposes, we're only doing 100 files here, which takes under 1 minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd2fa4-23ed-4d22-96ba-efcda3d959e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudcatalog\n",
    "import boto3\n",
    "import dask\n",
    "import io\n",
    "import time\n",
    "import re\n",
    "import astropy.io.fits\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway, GatewayCluster\n",
    "    \n",
    "def DO_SCIENCE(mydata):\n",
    "    # Calculates total irradiance from an image.  You can put better science here\n",
    "    iirad = mydata.mean()\n",
    "    return iirad\n",
    "\n",
    "def process_fits_s3(s3key:str): # -> Tuple[str, float]:\n",
    "    \"\"\" grabs an S3 file then runs DO_SCIENCE() on it \"\"\"\n",
    "    sess = boto3.session.Session() # do this each open to avoid thread problem 'credential_provider'\n",
    "    s3c = sess.client(\"s3\")\n",
    "    [mybucket,mykey] = re.sub(r\"s3://\",\"\",s3key).split(\"/\",1)\n",
    "    try:\n",
    "        fobj = s3c.get_object(Bucket=mybucket,Key=mykey)\n",
    "        rawdata = fobj['Body'].read()\n",
    "        bdata = io.BytesIO(rawdata)\n",
    "        hdul = astropy.io.fits.open(bdata,memmap=False)        \n",
    "        date = hdul[1].header['T_OBS']\n",
    "        irrad = DO_SCIENCE(hdul[1].data)\n",
    "    except:\n",
    "        print(\"Error fetching \",s3key)\n",
    "        date, irrad = None, None       \n",
    "    return date, irrad\n",
    "\n",
    "frID = \"aia_0094\"\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "file_registry1 = fr.request_cloud_catalog(frID, start_date=start, stop_date=stop, overwrite=False)\n",
    "filelist = file_registry1['datakey'].to_list()\n",
    "s3_files = filelist[0:100] # small test set to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d0955-f396-480c-b047-ec68c89dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now we initialize the Dask gateway and cluster, using your above parameters, to set up the virtual machines that will subsequently operate on the data. We use some non-optimized Dask parameters for setting up the run.\"\"\"\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores, options.worker_memory = 2, 1\n",
    "cluster = gateway.new_cluster(options)\n",
    "client=Client(cluster)\n",
    "cluster.adapt(minimum=1, maximum=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2efc6-7a81-4866-8596-b69130b4cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "now=time.time()\n",
    "try:\n",
    "    time_irrad = client.map(process_fits_s3, s3_files) # do it\n",
    "    all_data = client.gather(time_irrad) # gather results\n",
    "    print(\"Done! Completed in time \",(time.time()-now)/60.0,\"minutes, on\",len(all_data),\"files\")\n",
    "except:\n",
    "    print(\"Cluster needs to be started (or re-started) before running code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e6096-098c-439d-a5d3-07e6d62a02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plotme = [a for a in all_data if a[0] is not None]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot_date(*zip(*plotme),markersize=2)\n",
    "ax.set_xticks(ax.get_xticks()[::10], ax.get_xticklabels()[::10], rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937827c9-bb59-496d-8684-f1b1b31f9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "yn=input(\"Do you want to shut down the cluster, or re-use it? (y/r)?\")\n",
    "if yn == 'y':\n",
    "    cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c65cc-b9db-4122-811a-e484b93f6236",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "To recap, HelioCloud lets you do the same science as on your laptop using the same Python packages you already use, but in the cloud. This means a stable pre-loaded Python environment, fast access to Terabyte- and Petabyte-sized datasets, and the ability to throw lots of CPUs at data- or computation-heavy problems. Because everyone using a HelioCloud has the same software environment and data access rights, you can collaborate more easily because you only need to share code, not envs and datasets.\n",
    "\n",
    "The HelioCloud 'stack' runs on AWS and is released as open source, so any institution can install it.  It is available via heliocloud.org.  HelioCloud is a NASA-funded software development effort and eagerly seeks community input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33b37c-ce4a-4e01-bb8e-834feb62f9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
