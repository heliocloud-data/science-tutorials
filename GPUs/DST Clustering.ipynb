{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6e4989",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DST Clustering\n",
    "An attempt to use HAPI-NN to determine cluster different DST events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45af8a77-0004-4529-8ee1-cddcb61c2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import hapi_nn\n",
    "except:\n",
    "    %pip install hapi_nn\n",
    "    import os\n",
    "    os._exit(00)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8bfd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Import HAPI and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b559061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 16:21:13.680123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 16:21:13.776434: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from hapiclient import hapi\n",
    "from datetime import datetime\n",
    "from hapi_nn import HAPINNTrainer, HAPINNTester, config\n",
    "import hapi_nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0587ffa-5cf7-4671-bea6-552a2f3d2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df6b9b-6db9-431e-aaae-4a49941d1d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HAPI Setup\n",
    "Set HAPI related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba4ce64-1409-43f7-b1d2-89bfe58236cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.MODEL_ENGINE = 'TENSORFLOW'\n",
    "\n",
    "server = 'https://cdaweb.gsfc.nasa.gov/hapi'\n",
    "dataset = 'OMNI2_H0_MRG1HR'\n",
    "#Provisional Dst (2015/001-2020/366)\n",
    "start = '2016-01-01T00:00:00Z'\n",
    "stop = '2020-01-01T00:00:00Z'\n",
    "\n",
    "start2 = '2015-01-01T00:00:00Z'\n",
    "stop2 = '2016-01-01T00:00:00Z'\n",
    "\n",
    "parameters = 'DST1800'\n",
    "options = {'logging': True, 'usecache': True, 'cachedir': './hapicache'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb35f1c-5144-494d-9006-72234172e796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create HAPI-NN Trainer and Tester\n",
    "Begin testing HAPI-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556a4ed-41b7-42d7-b558-c359ce76f2e0",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45629a58-186a-4d85-a8bd-b1c6cbeb1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the period size has effect on the duration of events that can be clustered\n",
    "in_steps = 24 * 4  # 4 days\n",
    "out_steps = in_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8d507-8b17-49fe-8794-bc164e1da4c6",
   "metadata": {},
   "source": [
    "### Create Trainer and Tester as well as load data with HAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5caa4a-fbc8-41d4-8fe0-bc74ffaea592",
   "metadata": {},
   "source": [
    "Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139758d9-be16-4128-8f92-63066325d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = (.7, .2, .1)  # Train, Validation, Test\n",
    "    \n",
    "trainer = HAPINNTrainer(\n",
    "    splits, in_steps, out_steps,\n",
    "    preprocess_func=None,\n",
    "    lag=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61b620-a667-408e-8e32-05932e57c026",
   "metadata": {},
   "source": [
    "Load data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f654d-6f7e-45f6-b188-7efa1945eaa2",
   "metadata": {},
   "source": [
    "Model input will come from scalar and vector in HAPI dataset. The output comes from the first element in the column.\n",
    "The input is lagged behind the output, so we are forecasting the outputs based on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc6ad35-7834-4f8b-a64e-35675309cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hapi(): Running hapi.py version 0.2.5\n",
      "hapi(): file directory = ./hapicache/cdaweb.gsfc.nasa.gov_hapi\n",
      "hapi(): Reading OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.pkl\n",
      "hapi(): Reading OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.npy \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, meta = hapi(server, dataset, parameters, start, stop, **options)\n",
    "trainer.set_hapidatas([data], xyparameters=[['DST1800'], ['DST1800']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaf0567-3db5-4ad6-a4f1-3baf1601186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57245849-05f1-49fb-8f67-11b67170346e",
   "metadata": {},
   "source": [
    "Prepare the downloaded data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d5f4b9-5d1a-49ee-a314-240cf1ec99fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py:361: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(data)\n",
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py:372: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(remerge_data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7075194163332549, 0.1950164744645799, 0.09746410920216522)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc627349-8b80-441e-9ab5-3abb21f78b41",
   "metadata": {},
   "source": [
    "Test saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a908fe-80f3-440b-80af-41c27646123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_prepared_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abed8cb-845b-47b4-a8b4-536fcaeba27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_prepared_data('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090267f3-f6fe-4d6a-94bd-adc6604180b9",
   "metadata": {},
   "source": [
    "Create Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a26dd4c-07af-4b02-9f66-a847dd877665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_handle_2_outputs(x):\n",
    "    # Tester is used for full and encoder model\n",
    "    # Tester cannot handle seperate output heads, handling here\n",
    "    if isinstance(x, np.ndarray):\n",
    "        # handle numpy case or normal model case\n",
    "        return x\n",
    "    else:\n",
    "        # handle encoder case with two outputs, take first\n",
    "        return x[0]\n",
    "\n",
    "tester = HAPINNTester(\n",
    "    in_steps, out_steps, preprocess_func=None,\n",
    "    preprocess_y_func=custom_handle_2_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a53ea4-7ec8-4b45-8eb5-1f0288ca24f7",
   "metadata": {},
   "source": [
    "Load data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d4f1eed-81bb-468f-8ba6-17dc4f5e9301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hapi(): Running hapi.py version 0.2.5\n",
      "hapi(): file directory = ./hapicache/cdaweb.gsfc.nasa.gov_hapi\n",
      "hapi(): Reading OMNI2_H0_MRG1HR_DST1800_20150101T000000_20160101T000000.pkl\n",
      "hapi(): Reading OMNI2_H0_MRG1HR_DST1800_20150101T000000_20160101T000000.npy \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, meta = hapi(server, dataset, parameters, start2, stop2, **options)\n",
    "tester.set_hapidatas([data], xyparameters=[['DST1800'], ['DST1800']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3109477d-bcd7-4d6e-8fc5-0a1f7ee303ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e4b0d-7390-44f2-b889-bebd42004d8f",
   "metadata": {},
   "source": [
    "Prepare data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78b5c328-df3e-4e8a-b4b2-9a1e6fba25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da9ec6-7f9a-42df-8832-f4f8ed6d9c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec270144-30b6-4cf4-8c23-d578aa094875",
   "metadata": {},
   "source": [
    "Create TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d844497e-8a0c-4cb2-8c16-d8a3bc6d795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher compression ratio the more difficult reconstructing the input will be\n",
    "compression_ratio = 6\n",
    "compression = in_steps // compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e97987b-4c2d-477f-b18f-1eaafcfadc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(keras.Model):\n",
    "    def __init__(self, encoder, decoder, rloss_coef=1000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rloss_coef = rloss_coef\n",
    "        self.decoder.compiled_loss.build(\n",
    "            tf.zeros(self.decoder.output_shape[1:])\n",
    "        )\n",
    "        self.rloss = self.decoder.compiled_loss._losses[0]\n",
    "        \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            x = x[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "            z = eps * tf.exp(z_log_var * .5) + z_mean\n",
    "            reconstruction_loss = self.rloss.fn(\n",
    "                x, self.decoder(z, training=True)\n",
    "            )\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                reconstruction_loss\n",
    "            )\n",
    "            divergence_loss = .5 * tf.reduce_mean(\n",
    "                tf.exp(z_log_var) + z_mean**2 - 1. - z_log_var\n",
    "            )\n",
    "            loss = reconstruction_loss * self.rloss_coef + divergence_loss\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'divergence_loss': divergence_loss\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var = self.encoder(inputs, training=training)\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = eps * tf.exp(z_log_var * .5) + z_mean\n",
    "        y = self.decoder(z, training=training)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ccfa84-476b-43da-95be-acf7bcdcb83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 96, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 48, 16)       96          ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 24, 32)       2592        ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 12, 64)       10304       ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 6, 128)       41088       ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 3, 128)       49280       ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 1, 256)       98560       ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 256)          0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           4112        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           4112        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 16)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 210,144\n",
      "Trainable params: 210,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 16:21:51.328946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.369085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.369355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.369852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 16:21:51.370299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.370518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.370670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.886542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.886776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.886939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-11-21 16:21:51.887076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 325 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               2176      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 3, 256)           98560     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 6, 128)           98432     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 12, 128)          82048     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_3 (Conv1DT  (None, 24, 64)           41024     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_4 (Conv1DT  (None, 48, 32)           10272     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_5 (Conv1DT  (None, 96, 16)           2576      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_6 (Conv1DT  (None, 96, 1)            17        \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335,105\n",
      "Trainable params: 335,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x0 = keras.layers.Input(shape=(in_steps, 1))\n",
    "x = keras.layers.Conv1D(compression, 5, strides=2, padding='same', activation='swish')(x0)\n",
    "x = keras.layers.Conv1D(compression * 2, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 4, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 8, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 8, 3, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 16, 3, strides=3)(x)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "z_mean = keras.layers.Dense(compression)(x)\n",
    "z_log_var = keras.layers.Dense(compression)(x)\n",
    "z_log_var = keras.layers.Lambda(\n",
    "    lambda x: tf.clip_by_value(x, -50, 50)  # helps to avoid NaN\n",
    ")(z_log_var)\n",
    "encoder_model = keras.Model(inputs=x0, outputs=[z_mean, z_log_var])\n",
    "encoder_model.compile(\n",
    "    optimizer='adam', loss='mse', metrics=['mae']\n",
    ")\n",
    "encoder_model.summary()\n",
    "\n",
    "x0 = keras.layers.Input(shape=(compression,))\n",
    "x = keras.layers.Dense(compression * 8)(x0)\n",
    "x = keras.layers.Reshape((1, compression * 8))(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 16, 3, strides=3)(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 8, 3, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 8, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 4, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 2, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(1, 1, strides=1, padding='same')(x)\n",
    "decoder_model = keras.Model(inputs=x0, outputs=x)\n",
    "decoder_model.compile(optimizer=keras.optimizers.Adam(.001), loss='mse', metrics=['mae'])\n",
    "decoder_model.summary()\n",
    "\n",
    "model = VAEModel(encoder_model, decoder_model, rloss_coef=2)\n",
    "model.compile(optimizer=decoder_model.optimizer,\n",
    "              loss=decoder_model.loss,\n",
    "              metrics=decoder_model.compiled_metrics._metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223e7d1-678d-4403-8fc0-c4fd522fe9ec",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ed2e3c-d478-4e19-bcb5-2eab0c0145d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 16:21:58.504447: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-11-21 16:21:58.504825: E tensorflow/stream_executor/cuda/cuda_dnn.cc:398] Possibly insufficient driver version: 550.90.7\n",
      "2024-11-21 16:21:58.504870: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at conv_ops.cc:1134 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/srv/conda/envs/notebook/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/srv/conda/envs/notebook/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1197/3479292571.py\", line 4, in <module>\n      trainer.train(model, epochs, batch_size=batch_size)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py\", line 668, in train\n      results = self.tf_train(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py\", line 609, in tf_train\n      model.fit(self.processed_data['train_x'],\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/tmp/ipykernel_1197/2435385333.py\", line 17, in train_step\n      z_mean, z_log_var = self.encoder(x, training=True)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_3123]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py:668\u001b[0m, in \u001b[0;36mHAPINNTrainer.train\u001b[0;34m(self, model, epochs, batch_size, loss_func, metric_func, optimizer, device, verbose)\u001b[0m\n\u001b[1;32m    657\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_train(\n\u001b[1;32m    658\u001b[0m         model,\n\u001b[1;32m    659\u001b[0m         loss_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m    666\u001b[0m     )\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py:609\u001b[0m, in \u001b[0;36mHAPINNTrainer.tf_train\u001b[0;34m(self, model, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_x\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData must first be partitioned.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 609\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_x\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_x\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/conv1d/Conv1D' defined at (most recent call last):\n    File \"/srv/conda/envs/notebook/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/srv/conda/envs/notebook/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/srv/conda/envs/notebook/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1197/3479292571.py\", line 4, in <module>\n      trainer.train(model, epochs, batch_size=batch_size)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py\", line 668, in train\n      results = self.tf_train(\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/hapi_nn/training.py\", line 609, in tf_train\n      model.fit(self.processed_data['train_x'],\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/tmp/ipykernel_1197/2435385333.py\", line 17, in train_step\n      z_mean, z_log_var = self.encoder(x, training=True)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_3123]"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "trainer.train(model, epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027510d-54de-43d0-8a93-65cdf0a97c06",
   "metadata": {},
   "source": [
    "## Utilize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d8020-2875-49ac-b747-f26ae5067110",
   "metadata": {},
   "source": [
    "Predict over the downloaded testing data using the default stride (out_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fa89a-fc87-4567-833f-be722560a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tester.test(model)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80519e-de42-4e89-be11-89c6feca4a25",
   "metadata": {},
   "source": [
    "Display one window of the test data overlayed with the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20934dd-6b5d-406c-8577-0d021ab164ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.plot(predictions, 9, 'DST1800')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe09e5-78ef-4ca6-ae36-feaedbcc9c7a",
   "metadata": {},
   "source": [
    "Display all of the test data overlayed with the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a0265-7c84-4666-8fe3-97a9b2f984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.plot(predictions, -1, 'DST1800')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d503-df9f-46b3-8b08-ed7356afad61",
   "metadata": {},
   "source": [
    "Get encodings over the downloaded testing data using the default stride (out_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e68448-a309-4688-a731-4cf5ce8882b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tester.test(encoder_model)\n",
    "encodings = np.array(encodings)\n",
    "# note that these encoding values cannot be ploted using the tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa1f70-92bb-4f11-8526-a80f391fa390",
   "metadata": {},
   "source": [
    "Plot the encodings with TSNE and PCA since the encoding vectors have more than 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d2672-5731-419f-b6d3-ae23c4cdba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = encodings.reshape(\n",
    "    (encodings.shape[0], np.prod(encodings.shape[1:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93f585-3e42-446e-90cf-9588d60d1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, init='pca', learning_rate='auto')\n",
    "tsne_transformed = model.fit_transform(x_data)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d76ff2-6427-4d54-9af1-35fb16eb0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "pca_transformed = model.fit_transform(x_data)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(pca_transformed[:, 0], pca_transformed[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af581ef-7c3c-4b30-94a1-21d6891a0b6b",
   "metadata": {},
   "source": [
    "Cluster the test data using KMeans and GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db15e7-9a56-4adb-b938-1de29aba4c2c",
   "metadata": {},
   "source": [
    "KMeans has it's own transform, so plot clusters on that transform so long as only 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fb603-ecd1-4c4c-90c5-9b2eb6f40dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndxs = np.arange(len(encodings))\n",
    "pred_ndxs = np.floor(ndxs / tester.out_steps).tolist()\n",
    "time_ndxs = ndxs * 60 * 60 * tester.out_steps\n",
    "# assuming start date of test data is Jan. 1st, 2015\n",
    "dates = [datetime.utcfromtimestamp(x) for x in (time_ndxs + datetime(year=2015, month=1, day=1).timestamp())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961028d-bd29-4824-873a-f0e2248336c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "model.fit(x_data)\n",
    "transformed = model.fit_transform(x_data)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(transformed[:, 0], transformed[:, 1], c=model.labels_)\n",
    "for ndx in range(len(transformed)):\n",
    "    if model.labels_[ndx] == 1:\n",
    "        ax.annotate(dates[ndx], (transformed[ndx, 0], transformed[ndx, 1]))\n",
    "plt.show()\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75fd58d-783a-477b-8de2-adbb800c3566",
   "metadata": {},
   "source": [
    "Compute GaussianMixture and plot on PCA transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cfdb6-d803-4953-a96f-b187652e7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture(n_components=2)\n",
    "model.fit(x_data)\n",
    "labels = model.fit_predict(x_data)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(transformed[:, 0], transformed[:, 1], c=labels)\n",
    "for ndx in range(len(transformed)):\n",
    "    if labels[ndx] == 1:\n",
    "        ax.annotate(dates[ndx], (transformed[ndx, 0], transformed[ndx, 1]))\n",
    "plt.show()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290453d5-5b16-4169-afcc-9cb288b832fe",
   "metadata": {},
   "source": [
    "Focus on a few of the two clusters by plotting them individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93d895-a73e-492e-9013-58e8d39d87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 16))\n",
    "for ndx, label in enumerate(labels[:len(labels)//6]):\n",
    "    if label == 0:\n",
    "        tester.plot(predictions, ndx, 'DST1800')\n",
    "        plt.scatter(time_ndxs[ndx], 0, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70327c-bf4e-4b53-9842-a1f3213c9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 16))\n",
    "for ndx, label in enumerate(labels[:len(labels)//2]):\n",
    "    if label == 1:\n",
    "        tester.plot(predictions, ndx, 'DST1800')\n",
    "        plt.scatter(time_ndxs[ndx], 0, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905b9d9-29c0-497b-80d2-643d5d22551d",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0056e6d-cd51-476e-831f-59f5523235dd",
   "metadata": {},
   "source": [
    "For the most part there seems to be a pattern in the second cluster that differentiates the two clusters. This pattern is the presences of extreme dips near the end of the series. However, the first cluster does have a couple instances with a similar pattern but these have spikes upward before the dip in their series reconstruction. The differences in the clusters does not seem to perfectly split on CMEs or Flares, with both clusters having such events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02953c-acb0-4a26-a636-353d3a932e3c",
   "metadata": {},
   "source": [
    "©️ 2022 The Johns Hopkins University Applied Physics Laboratory LLC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
