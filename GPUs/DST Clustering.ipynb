{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6e4989",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DST Clustering\n",
    "An attempt to use HAPI-NN to determine cluster different DST events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45af8a77-0004-4529-8ee1-cddcb61c2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import hapi_nn\n",
    "except:\n",
    "    %pip install hapi_nn\n",
    "    import os\n",
    "    os._exit(00)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8bfd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "Import HAPI and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b559061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 15:18:58.455944: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 15:18:58.500915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-04 15:18:58.500934: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-04 15:18:58.500939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 15:18:58.507431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from hapiclient import hapi\n",
    "from datetime import datetime\n",
    "from hapi_nn import HAPINNTrainer, HAPINNTester, config\n",
    "import hapi_nn\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0587ffa-5cf7-4671-bea6-552a2f3d2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df6b9b-6db9-431e-aaae-4a49941d1d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HAPI Setup\n",
    "Set HAPI related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba4ce64-1409-43f7-b1d2-89bfe58236cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.MODEL_ENGINE = 'TENSORFLOW'\n",
    "\n",
    "server = 'https://cdaweb.gsfc.nasa.gov/hapi'\n",
    "dataset = 'OMNI2_H0_MRG1HR'\n",
    "#Provisional Dst (2015/001-2020/366)\n",
    "start = '2016-01-01T00:00:00Z'\n",
    "stop = '2020-01-01T00:00:00Z'\n",
    "\n",
    "start2 = '2015-01-01T00:00:00Z'\n",
    "stop2 = '2016-01-01T00:00:00Z'\n",
    "\n",
    "parameters = 'DST1800'\n",
    "options = {'logging': True, 'usecache': True, 'cachedir': './hapicache'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb35f1c-5144-494d-9006-72234172e796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create HAPI-NN Trainer and Tester\n",
    "Begin testing HAPI-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b556a4ed-41b7-42d7-b558-c359ce76f2e0",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45629a58-186a-4d85-a8bd-b1c6cbeb1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the period size has effect on the duration of events that can be clustered\n",
    "in_steps = 24 * 4  # 4 days\n",
    "out_steps = in_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8d507-8b17-49fe-8794-bc164e1da4c6",
   "metadata": {},
   "source": [
    "### Create Trainer and Tester as well as load data with HAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5caa4a-fbc8-41d4-8fe0-bc74ffaea592",
   "metadata": {},
   "source": [
    "Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139758d9-be16-4128-8f92-63066325d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = (.7, .2, .1)  # Train, Validation, Test\n",
    "    \n",
    "trainer = HAPINNTrainer(\n",
    "    splits, in_steps, out_steps,\n",
    "    preprocess_func=None,\n",
    "    lag=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61b620-a667-408e-8e32-05932e57c026",
   "metadata": {},
   "source": [
    "Load data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f654d-6f7e-45f6-b188-7efa1945eaa2",
   "metadata": {},
   "source": [
    "Model input will come from scalar and vector in HAPI dataset. The output comes from the first element in the column.\n",
    "The input is lagged behind the output, so we are forecasting the outputs based on the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc6ad35-7834-4f8b-a64e-35675309cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hapi(): Running hapi.py version 0.2.5\n",
      "hapi(): file directory = ./hapicache/cdaweb.gsfc.nasa.gov_hapi\n",
      "hapi(): Reading ./hapicache/cdaweb.gsfc.nasa.gov_hapi\n",
      "hapi(): Writing OMNI2_H0_MRG1HR___.json \n",
      "hapi(): Writing OMNI2_H0_MRG1HR___.pkl \n",
      "hapi(): Reading https://cdaweb.gsfc.nasa.gov/hapi/capabilities\n",
      "hapi(): Writing https://cdaweb.gsfc.nasa.gov/hapi/data?id=OMNI2_H0_MRG1HR&parameters=DST1800&time.min=2016-01-01T00:00:00Z&time.max=2020-01-01T00:00:00Z&format=binary to OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.bin\n",
      "hapi(): Reading and parsing OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.bin\n",
      "hapi(): Writing ./hapicache/cdaweb.gsfc.nasa.gov_hapi/OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.pkl\n",
      "hapi(): Writing ./hapicache/cdaweb.gsfc.nasa.gov_hapi/OMNI2_H0_MRG1HR_DST1800_20160101T000000_20200101T000000.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/hapiclient/hapitime.py:287: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  Time = pandas.to_datetime(Time, infer_datetime_format=True).tz_convert(tzinfo).to_pydatetime()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, meta = hapi(server, dataset, parameters, start, stop, **options)\n",
    "trainer.set_hapidatas([data], xyparameters=[['DST1800'], ['DST1800']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eaf0567-3db5-4ad6-a4f1-3baf1601186e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57245849-05f1-49fb-8f67-11b67170346e",
   "metadata": {},
   "source": [
    "Prepare the downloaded data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5f4b9-5d1a-49ee-a314-240cf1ec99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc627349-8b80-441e-9ab5-3abb21f78b41",
   "metadata": {},
   "source": [
    "Test saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a908fe-80f3-440b-80af-41c27646123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_prepared_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abed8cb-845b-47b4-a8b4-536fcaeba27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_prepared_data('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090267f3-f6fe-4d6a-94bd-adc6604180b9",
   "metadata": {},
   "source": [
    "Create Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26dd4c-07af-4b02-9f66-a847dd877665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_handle_2_outputs(x):\n",
    "    # Tester is used for full and encoder model\n",
    "    # Tester cannot handle seperate output heads, handling here\n",
    "    if isinstance(x, np.ndarray):\n",
    "        # handle numpy case or normal model case\n",
    "        return x\n",
    "    else:\n",
    "        # handle encoder case with two outputs, take first\n",
    "        return x[0]\n",
    "\n",
    "tester = HAPINNTester(\n",
    "    in_steps, out_steps, preprocess_func=None,\n",
    "    preprocess_y_func=custom_handle_2_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a53ea4-7ec8-4b45-8eb5-1f0288ca24f7",
   "metadata": {},
   "source": [
    "Load data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f1eed-81bb-468f-8ba6-17dc4f5e9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = hapi(server, dataset, parameters, start2, stop2, **options)\n",
    "tester.set_hapidatas([data], xyparameters=[['DST1800'], ['DST1800']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109477d-bcd7-4d6e-8fc5-0a1f7ee303ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e4b0d-7390-44f2-b889-bebd42004d8f",
   "metadata": {},
   "source": [
    "Prepare data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5c328-df3e-4e8a-b4b2-9a1e6fba25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da9ec6-7f9a-42df-8832-f4f8ed6d9c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec270144-30b6-4cf4-8c23-d578aa094875",
   "metadata": {},
   "source": [
    "Create TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844497e-8a0c-4cb2-8c16-d8a3bc6d795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher compression ratio the more difficult reconstructing the input will be\n",
    "compression_ratio = 6\n",
    "compression = in_steps // compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97987b-4c2d-477f-b18f-1eaafcfadc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(keras.Model):\n",
    "    def __init__(self, encoder, decoder, rloss_coef=1000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.rloss_coef = rloss_coef\n",
    "        self.decoder.compiled_loss.build(\n",
    "            tf.zeros(self.decoder.output_shape[1:])\n",
    "        )\n",
    "        self.rloss = self.decoder.compiled_loss._losses[0]\n",
    "        \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, (tuple, list)):\n",
    "            x = x[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(x, training=True)\n",
    "            eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "            z = eps * tf.exp(z_log_var * .5) + z_mean\n",
    "            reconstruction_loss = self.rloss.fn(\n",
    "                x, self.decoder(z, training=True)\n",
    "            )\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                reconstruction_loss\n",
    "            )\n",
    "            divergence_loss = .5 * tf.reduce_mean(\n",
    "                tf.exp(z_log_var) + z_mean**2 - 1. - z_log_var\n",
    "            )\n",
    "            loss = reconstruction_loss * self.rloss_coef + divergence_loss\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'divergence_loss': divergence_loss\n",
    "        }\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var = self.encoder(inputs, training=training)\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = eps * tf.exp(z_log_var * .5) + z_mean\n",
    "        y = self.decoder(z, training=training)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccfa84-476b-43da-95be-acf7bcdcb83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = keras.layers.Input(shape=(in_steps, 1))\n",
    "x = keras.layers.Conv1D(compression, 5, strides=2, padding='same', activation='swish')(x0)\n",
    "x = keras.layers.Conv1D(compression * 2, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 4, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 8, 5, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 8, 3, strides=2, padding='same', activation='swish')(x)\n",
    "x = keras.layers.Conv1D(compression * 16, 3, strides=3)(x)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "z_mean = keras.layers.Dense(compression)(x)\n",
    "z_log_var = keras.layers.Dense(compression)(x)\n",
    "z_log_var = keras.layers.Lambda(\n",
    "    lambda x: tf.clip_by_value(x, -50, 50)  # helps to avoid NaN\n",
    ")(z_log_var)\n",
    "encoder_model = keras.Model(inputs=x0, outputs=[z_mean, z_log_var])\n",
    "encoder_model.compile(\n",
    "    optimizer='adam', loss='mse', metrics=['mae']\n",
    ")\n",
    "encoder_model.summary()\n",
    "\n",
    "x0 = keras.layers.Input(shape=(compression,))\n",
    "x = keras.layers.Dense(compression * 8)(x0)\n",
    "x = keras.layers.Reshape((1, compression * 8))(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 16, 3, strides=3)(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 8, 3, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 8, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 4, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression * 2, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(compression, 5, strides=2, padding='same')(x)\n",
    "x = keras.layers.Conv1DTranspose(1, 1, strides=1, padding='same')(x)\n",
    "decoder_model = keras.Model(inputs=x0, outputs=x)\n",
    "decoder_model.compile(optimizer=keras.optimizers.Adam(.001), loss='mse', metrics=['mae'])\n",
    "decoder_model.summary()\n",
    "\n",
    "model = VAEModel(encoder_model, decoder_model, rloss_coef=2)\n",
    "model.compile(optimizer=decoder_model.optimizer,\n",
    "              loss=decoder_model.loss,\n",
    "              metrics=decoder_model.compiled_metrics._metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223e7d1-678d-4403-8fc0-c4fd522fe9ec",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed2e3c-d478-4e19-bcb5-2eab0c0145d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "trainer.train(model, epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027510d-54de-43d0-8a93-65cdf0a97c06",
   "metadata": {},
   "source": [
    "## Utilize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d8020-2875-49ac-b747-f26ae5067110",
   "metadata": {},
   "source": [
    "Predict over the downloaded testing data using the default stride (out_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fa89a-fc87-4567-833f-be722560a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tester.test(model)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80519e-de42-4e89-be11-89c6feca4a25",
   "metadata": {},
   "source": [
    "Display one window of the test data overlayed with the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20934dd-6b5d-406c-8577-0d021ab164ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.plot(predictions, 9, 'DST1800')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe09e5-78ef-4ca6-ae36-feaedbcc9c7a",
   "metadata": {},
   "source": [
    "Display all of the test data overlayed with the prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a0265-7c84-4666-8fe3-97a9b2f984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.plot(predictions, -1, 'DST1800')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1d503-df9f-46b3-8b08-ed7356afad61",
   "metadata": {},
   "source": [
    "Get encodings over the downloaded testing data using the default stride (out_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e68448-a309-4688-a731-4cf5ce8882b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tester.test(encoder_model)\n",
    "encodings = np.array(encodings)\n",
    "# note that these encoding values cannot be ploted using the tester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa1f70-92bb-4f11-8526-a80f391fa390",
   "metadata": {},
   "source": [
    "Plot the encodings with TSNE and PCA since the encoding vectors have more than 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d2672-5731-419f-b6d3-ae23c4cdba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = encodings.reshape(\n",
    "    (encodings.shape[0], np.prod(encodings.shape[1:]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93f585-3e42-446e-90cf-9588d60d1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, init='pca', learning_rate='auto')\n",
    "tsne_transformed = model.fit_transform(x_data)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(tsne_transformed[:, 0], tsne_transformed[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d76ff2-6427-4d54-9af1-35fb16eb0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "pca_transformed = model.fit_transform(x_data)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(pca_transformed[:, 0], pca_transformed[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af581ef-7c3c-4b30-94a1-21d6891a0b6b",
   "metadata": {},
   "source": [
    "Cluster the test data using KMeans and GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db15e7-9a56-4adb-b938-1de29aba4c2c",
   "metadata": {},
   "source": [
    "KMeans has it's own transform, so plot clusters on that transform so long as only 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fb603-ecd1-4c4c-90c5-9b2eb6f40dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndxs = np.arange(len(encodings))\n",
    "pred_ndxs = np.floor(ndxs / tester.out_steps).tolist()\n",
    "time_ndxs = ndxs * 60 * 60 * tester.out_steps\n",
    "# assuming start date of test data is Jan. 1st, 2015\n",
    "dates = [datetime.utcfromtimestamp(x) for x in (time_ndxs + datetime(year=2015, month=1, day=1).timestamp())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961028d-bd29-4824-873a-f0e2248336c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "model.fit(x_data)\n",
    "transformed = model.fit_transform(x_data)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(transformed[:, 0], transformed[:, 1], c=model.labels_)\n",
    "for ndx in range(len(transformed)):\n",
    "    if model.labels_[ndx] == 1:\n",
    "        ax.annotate(dates[ndx], (transformed[ndx, 0], transformed[ndx, 1]))\n",
    "plt.show()\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75fd58d-783a-477b-8de2-adbb800c3566",
   "metadata": {},
   "source": [
    "Compute GaussianMixture and plot on PCA transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cfdb6-d803-4953-a96f-b187652e7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianMixture(n_components=2)\n",
    "model.fit(x_data)\n",
    "labels = model.fit_predict(x_data)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(transformed[:, 0], transformed[:, 1], c=labels)\n",
    "for ndx in range(len(transformed)):\n",
    "    if labels[ndx] == 1:\n",
    "        ax.annotate(dates[ndx], (transformed[ndx, 0], transformed[ndx, 1]))\n",
    "plt.show()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290453d5-5b16-4169-afcc-9cb288b832fe",
   "metadata": {},
   "source": [
    "Focus on a few of the two clusters by plotting them individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93d895-a73e-492e-9013-58e8d39d87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 16))\n",
    "for ndx, label in enumerate(labels[:len(labels)//6]):\n",
    "    if label == 0:\n",
    "        tester.plot(predictions, ndx, 'DST1800')\n",
    "        plt.scatter(time_ndxs[ndx], 0, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d70327c-bf4e-4b53-9842-a1f3213c9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 16))\n",
    "for ndx, label in enumerate(labels[:len(labels)//2]):\n",
    "    if label == 1:\n",
    "        tester.plot(predictions, ndx, 'DST1800')\n",
    "        plt.scatter(time_ndxs[ndx], 0, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905b9d9-29c0-497b-80d2-643d5d22551d",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0056e6d-cd51-476e-831f-59f5523235dd",
   "metadata": {},
   "source": [
    "For the most part there seems to be a pattern in the second cluster that differentiates the two clusters. This pattern is the presences of extreme dips near the end of the series. However, the first cluster does have a couple instances with a similar pattern but these have spikes upward before the dip in their series reconstruction. The differences in the clusters does not seem to perfectly split on CMEs or Flares, with both clusters having such events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02953c-acb0-4a26-a636-353d3a932e3c",
   "metadata": {},
   "source": [
    "©️ 2022 The Johns Hopkins University Applied Physics Laboratory LLC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
