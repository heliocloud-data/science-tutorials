{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edac1c14-2f71-4434-a3d3-ef4cfefddd6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HelioCloud Storage + Burst + SDO\n",
    "\n",
    "* HelioCloud is an AWS Cloud environment that is user-friendly\n",
    "* Daskhub is a cloud Notebooks setup that allows parallel processing via Dask\n",
    "* Dask lets you temporarily throw lots of CPUs at a problem\n",
    "* S3 is the big cheap AWS storage\n",
    "* SDO is a mission with lots of image data\n",
    "\n",
    "Here we combine a few HelioCloud demos with our big data fileRegistry to tackle 1 year of SDO data.  The data is in AWS S3 and we do not copy the files over, but instead have CPUs at AWS access it directly\n",
    "\n",
    "* 1 year of SDO 94A EUV images from AIA is 129,758 files, each 14MB, totalling 1.8 TB.\n",
    "* This code calculates a simple irradiance\n",
    "* If done serially on your laptop, it would take 27 hours\n",
    "* HelioCloud takes 25 minutes (1467 sec) to analyze through 1 year of SDO data\n",
    "\n",
    "(More fun stats: that's 88 files/second, also 1 GB/sec to do the full analysis (which is 2x the read speed of a SATA SSD.  It is 2x faster than it would take to just copy the files off a local disk and 8x faster than copying via gigabit internet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92ddc5-b67f-419b-bce9-7e1192859132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cloudcatalog\n",
    "import boto3\n",
    "import dask\n",
    "import io\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import astropy.io.fits\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway, GatewayCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afebbf7-65cc-4aa7-b0a4-be911f5e43c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HelioCloud shared cloud file registry (cloudcatalog)\n",
    "\n",
    "This is a simple standard for any dataset that enables users to access it via an API or directly.  The short definition is:\n",
    "    * S3 disks have a 'catalog.json' describing their datasets\n",
    "    * Each dataset has a <dataid>_YYYY.csv index file of its contents\n",
    "    * These indexes have the form \"time, s3_location, filesize\" (plus optional metadata)\n",
    "    \n",
    "Let's walk through this.\n",
    "\n",
    "# List of all known 'catalogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f7fc2-f324-48ae-9d21-86150f9b68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is pre-release so the main registry is on a temporary site\n",
    "cr=cloudcatalog.CatalogRegistry()\n",
    "\n",
    "# Let us see which clouds are known to the HelioCloud network\n",
    "cat = cr.get_catalog()\n",
    "print(\"List of clouds via get_catalog\",cat,'\\n')\n",
    "reg = cr.get_registry()\n",
    "print(\"Or just the S3 endpoints via get_registry:\",reg,'\\n')\n",
    "link = cr.get_entries()\n",
    "print(\"get_entries:\",link,'\\n')\n",
    "\n",
    "# Now let us grab the specific cloud containing SDO\n",
    "url = cr.get_endpoint(\"HelioCloud, including SDO\")\n",
    "print(\"SDO result for get_endpoint:\",url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fba2aa-ca04-40e2-bb2e-935e50d2b3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## For a given S3 location, print the first in the list of all available datasets\n",
    "\n",
    "If you want to print the entire list, change the variable \"show_all\" to True. Be warned that the list will be extremely long and difficult to parse. Remember that you can right click on the cell and select \"Clear Cell Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e801f1c-eecf-45be-9dc4-46b0dc32eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all = False # True\n",
    "\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "\n",
    "if show_all:\n",
    "    print(\"All datasets at this HDRL S3:\\n\",fr.get_entries_dict())\n",
    "else:\n",
    "    print(\"First entry in the list of all datasets at this HDRL S3:\\n\",fr.get_entries_dict()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed2aeed-e1c9-44e0-9b6a-9406d593ce21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Print metadata associated with AIA 94A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbee047-a578-4a09-8199-e21473f04f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "frID = \"aia_0094\"\n",
    "myjson = fr.get_entry(frID)\n",
    "start, stop = myjson['start'], myjson['stop']\n",
    "print(frID,start,stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af29310-92ab-42f3-9f4d-dfdf7eea45b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optionally you can choose a subsetted time range (faster testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bde4f-6c15-4b42-9b5b-73740eef9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = True\n",
    "if subset: start, stop = '2020-01-01T00:00:00Z', '2020-12-31T23:59:59Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4b1f3-3dab-4e57-b7a4-1ba2774ee447",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get the entire list of SDO files for that AIA ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15581bf1-cebd-4ad5-8100-f5a0fb08f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_registry1 = fr.request_cloud_catalog(frID, start_date=start, stop_date=stop, overwrite=False)\n",
    "# And convert that richer data to a list of files to process\n",
    "filelist = file_registry1['datakey'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b42f9-e29c-4385-9aa3-decb3b8b4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_registry1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793c347-306e-4311-811b-f44e66056786",
   "metadata": {},
   "source": [
    "# Actual Analysis code\n",
    "\n",
    "Here's our analysis routines, that operate on a single FITS file and returns the results of our 'work_on_data()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd2fa4-23ed-4d22-96ba-efcda3d959e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DO_SCIENCE(mydata):\n",
    "    # you can put better science here\n",
    "    science_result = mydata.mean()\n",
    "    return science_result\n",
    "\n",
    "# these are variable helpful handler functions\n",
    "def s3url_to_bucketkey(s3url: str): # -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts the S3 bucket name and file key from an S3 URL.\n",
    "    e.g. s3://mybucket/mykeypart1/mykeypart2/fname.fits -> mybucket, mykeypart1/mykeypart2/fname.fits\n",
    "    \"\"\"\n",
    "    name2 = re.sub(r\"s3://\",\"\",s3url)\n",
    "    s = name2.split(\"/\",1)\n",
    "    return s[0], s[1]\n",
    "\n",
    "def process_fits_s3(s3key:str): # -> Tuple[str, float]:\n",
    "    \"\"\" For a single FITS file, read it from S3, grab the header and\n",
    "        data, then do the DO_SCIENCE() call of choice\n",
    "    \"\"\"\n",
    "    sess = boto3.session.Session() # do this each open to avoid thread problem 'credential_provider'\n",
    "    s3c = sess.client(\"s3\")\n",
    "    mybucket,mykey = s3url_to_bucketkey(s3key)\n",
    "    try:\n",
    "        fobj = s3c.get_object(Bucket=mybucket,Key=mykey)\n",
    "        rawdata = fobj['Body'].read()\n",
    "        bdata = io.BytesIO(rawdata)\n",
    "        hdul = astropy.io.fits.open(bdata,memmap=False)        \n",
    "        date = hdul[1].header['T_OBS']\n",
    "        science_result = DO_SCIENCE(hdul[1].data)\n",
    "    except:\n",
    "        print(\"Error fetching \",s3key)\n",
    "        date, science_result = None, None\n",
    "        \n",
    "    return date, science_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf0efe-2e6d-4a5a-84aa-fc6cebe5f690",
   "metadata": {},
   "source": [
    "# Accessing S3 from anywhere\n",
    "The power of cloud is moving compute (CPUs) to the data, instead of hauling data over internet.  But if you want to play, here is a serial example of fetching the first 10 SDO files and processing them anywhere.  It is serial and slow-- roughly 66x slower than using parallel processing and 128x or more slow if you are fetching from S3 to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cae5c-a6e8-4511-8ce5-ac9c7abcfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial test it on the first ten files, so slow\n",
    "now=time.time()\n",
    "for i in range(10):\n",
    "    results = process_fits_s3(filelist[i])\n",
    "    print(results)\n",
    "print(time.time()-now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffeb6a-2900-412c-a9ba-16df048739ae",
   "metadata": {},
   "source": [
    "# Setting up the Dask 'burst' Configuration\n",
    "This is pretty standard daskhub configuration from the HelioCloud dask demos, that works to burst tasks into the cloud.\n",
    "\n",
    "We default to setting testing as 'True' so it only runs on 10,000 SDO files. If you set it to 'False', it will go through all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418677a-4af1-44a8-beed-67e8a4bb9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = True\n",
    "if testing:\n",
    "    s3_files = filelist[0:1000] # small test set to test\n",
    "else:\n",
    "    s3_files = filelist\n",
    "print(len(s3_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99371e30-4f54-4bdf-bccc-f9cf4d1cb8be",
   "metadata": {},
   "source": [
    "Here are some Dask parameters for setting up your run.  These are non-optimized values, feel free to play with them to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e5175-4ab9-4f61-8cb5-1049dd124c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of workers to use, for automatic scaling, our max number\n",
    "n_workers = 10 # 10-50 works well, more workers actually went slower\n",
    "# memory per worker (in Gb), typically 1, 2 or 4GB\n",
    "w_memory = 4\n",
    "# cores per worker, must be 1-4\n",
    "w_cores = 4\n",
    "# use Manual (if False, then uses Automatic scaling)\n",
    "use_manual_scaling = False\n",
    "\n",
    "useGUI = False # set true if you want the Dask widget as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b1c5-b8e4-4810-9f75-de85c3b9d25a",
   "metadata": {},
   "source": [
    "Now we initialzie the Dask gateway and cluster, using your above parameters, to set up the virtual machines that will subsequently operate on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d0955-f396-480c-b047-ec68c89dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the daskhub tutorial, setting up dask\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = w_cores\n",
    "options.worker_memory = w_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a6dcb-aa6b-4967-b394-2bc661c81b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize cluster and create client, takes < 15 seconds\n",
    "\n",
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client() # can also use 'client=Client(cluster)'\n",
    "cluster.adapt(minimum=1, maximum=n_workers)\n",
    "\n",
    "# This calls the widget\n",
    "if useGUI: cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92704361-2ce8-417c-9446-76758dd6b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client # let us take a look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396fd88-0e2f-49a6-8bcb-b64866affec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now the actual algorithm work and runtime\n",
    "\n",
    "In this case, our earlier 'file_registry1' is our set of fully qualified s3:// objects.  This code takes around 5 minutes for 10,000 files and 25 minutes for the full 130K files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2efc6-7a81-4866-8596-b69130b4cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "now=time.time()\n",
    "\n",
    "# simple version step 1, do it\n",
    "time_irrad = client.map(process_fits_s3, s3_files)\n",
    "\n",
    "# optional spot-check that jobs were sent\n",
    "print(time_irrad[0:4])\n",
    "\n",
    "# simple version step 2, gather results\n",
    "all_data = client.gather(time_irrad)\n",
    "\n",
    "print(\"Done! Completed in time \",(time.time()-now)/60.0,\"minutes, on\",len(all_data),\"files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ef944-39eb-40ae-8057-5e4012b66835",
   "metadata": {},
   "source": [
    "Remember to always shut down your cluster (so you are not spending $ on unused resources.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937827c9-bb59-496d-8684-f1b1b31f9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always shutdown\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24052d38-fa4d-424d-9412-4e43edc99b00",
   "metadata": {},
   "source": [
    "# Done, now do something with the results\n",
    "Time to save your results, plot it, and carry on with your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a87b88-fe2a-44da-9d64-fcea1f1a8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us save this\n",
    "with open('SDO_test-big.pickle','wb') as fout:\n",
    "    pickle.dump(all_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea4b37-e3f1-441d-8399-2dc72a50e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goodness check-- did any files not process?\n",
    "a = [a for a in all_data if a[0] is None]\n",
    "print(\"Bad fields: \",len(a))\n",
    "plotme = [a for a in all_data if a[0] is not None]\n",
    "print(\"Good fields: \",len(plotme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5340e2d-c271-4df1-a82f-cc053032e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "# Have Matplotlib create vector (svg) instead of raster (png) images\n",
    "#%config InlineBackend.figure_formats = ['svg'] \n",
    "#plt.figure()\n",
    "plt.plot_date(*zip(*plotme))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fb4a5-0584-491d-a649-d9e2b2b7e46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dce13-2246-4686-9d1c-9d8a04593f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
