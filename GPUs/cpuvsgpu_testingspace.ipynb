{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fee75c-58c5-45fa-9a62-8e96aa69e255",
   "metadata": {},
   "source": [
    "# GPU vs. CPU Time Testing\n",
    "\n",
    "In this notebook, we compare the speed at which the CPU and the GPU complete a matrix multiplication of the same random arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce3e81b-dc8b-47b6-b093-45bca457eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[761.879   749.5151  734.01904 ... 743.2405  739.74866 758.67645]\n",
      " [747.34094 734.46704 718.1686  ... 744.4136  739.31195 759.34924]\n",
      " [745.00195 742.61707 724.45233 ... 756.0435  750.24335 753.20526]\n",
      " ...\n",
      " [741.04034 735.8122  724.4512  ... 743.69775 745.3959  760.00024]\n",
      " [751.2343  738.1212  730.97296 ... 763.30334 757.3506  766.07104]\n",
      " [744.0852  744.8742  733.9725  ... 754.34326 752.95764 767.7954 ]], shape=(2000, 2000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# examples from https://www.tensorflow.org/guide/gpu\n",
    "\n",
    "\"\"\"\n",
    "If a TensorFlow operation has both CPU and GPU implementations, by default, the GPU device is prioritized when the \n",
    "operation is assigned. For example, tf.matmul has both CPU and GPU kernels and on a system with devices CPU:0 and \n",
    "GPU:0, the GPU:0 device is selected to run tf.matmul unless you explicitly request to run it on another device.\n",
    "\n",
    "If a TensorFlow operation has no corresponding GPU implementation, then the operation falls back to the CPU device. \n",
    "For example, since tf.cast only has a CPU kernel, on a system with devices CPU:0 and GPU:0, the CPU:0 device is \n",
    "selected to run tf.cast, even if requested to run on the GPU:0 device.\n",
    "\"\"\"\n",
    "\n",
    "# Matrices had to be in float32 format (test with float64?)\n",
    "# Make sure CPU time is faster than GPU time \n",
    "\n",
    "import os\n",
    "# Set log level to 2 to supress INFO and WARNING messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "array_a = np.random.rand(2000,3000).astype(np.float32)\n",
    "array_b = np.random.rand(3000,2000).astype(np.float32)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant(array_a)\n",
    "b = tf.constant(array_b)\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "#a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "#b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "#c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72af2284-bcc4-4ca6-adee-c1cc51cb633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "print(tf_build_info.build_info['cuda_version'])\n",
    "print(tf_build_info.build_info['cudnn_version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "403db0de-42c4-4d7d-85a2-a1cd1a80b0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _MklMatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[746.32587 743.61707 749.63763 ... 739.4483  749.2776  741.8144 ]\n",
      " [744.794   737.8059  749.718   ... 734.6162  747.1352  733.5072 ]\n",
      " [754.58905 750.89606 766.2672  ... 748.67426 757.71484 749.5813 ]\n",
      " ...\n",
      " [765.20435 753.2423  763.5108  ... 747.1719  760.8779  745.8576 ]\n",
      " [748.8022  744.7076  756.3685  ... 728.6443  746.9789  738.2501 ]\n",
      " [753.1909  740.11896 753.98145 ... 737.8197  755.7354  740.23474]], shape=(2000, 2000), dtype=float32)\n",
      "CPU times: user 387 ms, sys: 56.9 ms, total: 444 ms\n",
      "Wall time: 113 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 18:40:50.039102: I tensorflow/core/common_runtime/placer.cc:125] a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.039126: I tensorflow/core/common_runtime/placer.cc:125] b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.039132: I tensorflow/core/common_runtime/placer.cc:125] MatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.039136: I tensorflow/core/common_runtime/placer.cc:125] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.044634: I tensorflow/core/common_runtime/placer.cc:125] a: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.044654: I tensorflow/core/common_runtime/placer.cc:125] b: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.044660: I tensorflow/core/common_runtime/placer.cc:125] _MklMatMul: (_MklMatMul): /job:localhost/replica:0/task:0/device:CPU:0\n",
      "2025-04-28 18:40:50.044663: I tensorflow/core/common_runtime/placer.cc:125] product_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "  #a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  #b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "  a = tf.constant(array_a)\n",
    "  b = tf.constant(array_b)\n",
    "  c = tf.matmul(a, b)    \n",
    "\n",
    "# Run on the CPU\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fe1b22-78b2-46c4-8468-3830ef6ff07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[746.32556 743.61743 749.63763 ... 739.44836 749.2778  741.8149 ]\n",
      " [744.7941  737.8058  749.71783 ... 734.61664 747.1351  733.50757]\n",
      " [754.589   750.8961  766.26733 ... 748.6742  757.71497 749.58124]\n",
      " ...\n",
      " [765.20435 753.2423  763.511   ... 747.17175 760.8783  745.8574 ]\n",
      " [748.8022  744.70776 756.3685  ... 728.6443  746.979   738.25   ]\n",
      " [753.1911  740.1188  753.9816  ... 737.8197  755.7355  740.2344 ]], shape=(2000, 2000), dtype=float32)\n",
      "CPU times: user 36.3 ms, sys: 19.9 ms, total: 56.2 ms\n",
      "Wall time: 54.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Place tensors on the GPU\n",
    "with tf.device('/GPU:0'):\n",
    "  #a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  #b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "  a = tf.constant(array_a)\n",
    "  b = tf.constant(array_b)\n",
    "  c = tf.matmul(a, b) \n",
    "    \n",
    "# Run on the GPU\n",
    "#for i in range(2000):\n",
    "#  c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75095054-2bde-4c95-aa35-8d1b608408ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:44:20.861680: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-15 14:44:20.906330: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-15 14:44:20.906348: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-15 14:44:20.906353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-15 14:44:20.913384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "trying 1\n",
      "trying 2\n",
      "1 Physical GPU, 4 Logical GPUs\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 14:44:23.481294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.514528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.514766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.516860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.517079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.517231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.596299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.596520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.596668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.596810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.596955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.597103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.597210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "2025-04-15 14:44:23.597625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.597745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1024 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "2025-04-15 14:44:23.597865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.597969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 1024 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "2025-04-15 14:44:23.598225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-15 14:44:23.598337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 1024 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n"
     ]
    }
   ],
   "source": [
    "# Cut this out, rename this notebook \"something something time test\"\n",
    "\n",
    "# Table this until we do more with dask bursting of GPU servers\n",
    "\n",
    "\"\"\"\n",
    "If developing on a system with a single GPU, you can simulate multiple GPUs with virtual devices. \n",
    "This enables easy testing of multi-GPU setups without requiring additional resources.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  # Create 4 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    print('trying 1')\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n",
    "         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    print('trying 2')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    tf.debugging.set_log_device_placement(True)\n",
    "    gpus = tf.config.list_logical_devices('GPU')\n",
    "    strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "    with strategy.scope():\n",
    "        inputs = tf.keras.layers.Input(shape=(1,))\n",
    "        predictions = tf.keras.layers.Dense(1)(inputs)\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(loss='mse',\n",
    "                    optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n",
    "    print(\"Success.\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(\"Error:\",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd974ed4-1026-4dfb-af40-091ae297e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  /device:GPU:0\n",
      "Name:  /device:GPU:1\n",
      "Name:  /device:GPU:2\n",
      "Name:  /device:GPU:3\n",
      "tf.Tensor(\n",
      "[[ 88. 112.]\n",
      " [196. 256.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "if gpus:\n",
    "  # Replicate your computation on multiple GPUs\n",
    "  c = []\n",
    "  for gpu in gpus:\n",
    "    print(\"Name: \",gpu.name)\n",
    "    with tf.device(gpu.name):\n",
    "      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "      c.append(tf.matmul(a, b))\n",
    "\n",
    "  with tf.device('/GPU:0'):\n",
    "    matmul_sum = tf.add_n(c)\n",
    "\n",
    "  print(matmul_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec7ce4-9744-40ee-9bd8-a43a38ea4d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
