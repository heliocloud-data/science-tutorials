{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "SMCE Heliophysics DaskHub S3 Bucket Tutorial\n",
    "</H1>\n",
    "</CENTER>\n",
    "<!--<img src=\"./banner.jpg\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook to process data in SMCE helio-public S3 bucket using Dask\n",
    "\n",
    "<i>Note:  This document is maintained through the SMCE HelioCloud gitlab.</i>\n",
    "\n",
    "This is a simple example showing how to get a list of FITS files, run them through Dask workers to pull them from disk and examine the header keyword(s). \n",
    "\n",
    "You can use this notebook to test out various parameters you might feed to Dask; Consider the 'batch size', number of workers, number of cores per worker and memory per worker. Use the dashboard link to inspect how Dask is performing. Try both manual and automatic scaling strategies. See if you can get it to process 100 files in 20 sec or less!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## First:  What is S3? \n",
    "\n",
    "S3 stands for \"Simple Storage Service,\" which provides object storage for for AWS.  https://aws.amazon.com/s3/ \n",
    "\n",
    "It allows people to query and access data from a common location reference.  The buckets can be made <a href=\"https://stackoverflow.com/q/16784052\">web accessible to users outside of daskhub</a> if web access is enabled.    \n",
    "\n",
    "S3 buckets are individual storage elements. \n",
    "\n",
    "## Accessing S3 buckets\n",
    "\n",
    "To <a href= \"https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/ls.html\">get a list of the S3 buckets</a> on the SMCE Daskhub, enter this at a terminal prompt : <br>\n",
    "`aws s3 ls`\n",
    "\n",
    "To view the contents of a specific bucket, reference it with s3:// <br>\n",
    "`aws s3 ls s3://helio-public/`\n",
    ">            PRE SDO/\n",
    ">            PRE SOHO/\n",
    "\n",
    "(Note: \"PRE\" stands for prefix, so SDO/ is an AWS prefix with name SDO.) \n",
    "<hr>\n",
    "\n",
    "The external reference for this bucket is https://helio-public.s3.us-east-1.amazonaws.com/\n",
    "\n",
    "## Basic commands using S3 buckets\n",
    "\n",
    "To create a new directory, just reference it:<br>\n",
    "`aws s3 ls s3://helio-public/yourname/yourdir`\n",
    "\n",
    "then you can copy to the bucket as if it was a unix folder:<br>\n",
    "`aws s3 cp yourfile s3://helio-public/yourname/yourdir`\n",
    "\n",
    "copying multiple files is a bit more intricate, you need to put the multiple files in a directory first:<br>\n",
    "`aws s3 cp sourcedir/ s3://helio-public/yourname/yourdir --recursive`\n",
    "\n",
    "if you need access to a bucket that has restricted access, you have to run aws-mfa first:<br>\n",
    "`~/aws-mfa default`\n",
    "\n",
    "where default is a profile. To see available profiles:<br>\n",
    "`cat ~/.aws/credentials`\n",
    "\n",
    "you may need to change it to have execute permission first:<br>\n",
    "`chmod 755 ~/aws-mfa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (The code below needs to be commented for instructional purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "import dask\n",
    "import io\n",
    "import re\n",
    "import logging\n",
    "import s3fs\n",
    "\n",
    "from astropy.io import fits\n",
    "from dask.distributed import Client\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the bucket to upload to\n",
    "bucket_name = 'gov-nasa-hdrl-data1'\n",
    "\n",
    "# location in the bucket to use (a days worth of 211 A data from AIA on SDO for the date 2022-11-27)\n",
    "bucket_path = 'sdo/aia/20221127/0211/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Feel free to play with the following values to optimize the performance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of workers to use, for automatic scaling, our max number\n",
    "n_workers = 10\n",
    "\n",
    "# memory per worker (in Gb)\n",
    "w_memory = 2\n",
    "\n",
    "# cores per worker\n",
    "w_cores = 2\n",
    "\n",
    "# number of files to test against (360 max)\n",
    "n_files = 100\n",
    "\n",
    "# Number of files we release to be worked on by all workers at a time\n",
    "# the higher the number the more files being processed concurrently, but also\n",
    "# the greater the memory consumed. \n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the cluster and assign the client to the cluster, display the cluster widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway, GatewayCluster\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "\n",
    "# We're setting some defaults here just for grins... \n",
    "# I like the pangeo/base-notebook image for the workers since it has almost every library you'd need on a worker\n",
    "# In our environment, without setting these, the widget will default to the same image that the notebook itself is running, \n",
    "# as well as 2 cores and 4GB memory per worker\n",
    "\n",
    "options.worker_cores=w_cores\n",
    "options.worker_memory=w_memory\n",
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client()\n",
    "\n",
    "# use Manual (if False, then uses Automatic scaling)\n",
    "use_manual_scaling = False\n",
    "\n",
    "if use_manual_scaling:\n",
    "    # manual scaling (n_workers defined above)\n",
    "    cluster.scale(n_workers)\n",
    "else:\n",
    "    # Adaptively scale between 1 and n_workers (the max)\n",
    "    cluster.adapt(minimum=1, maximum=n_workers)\n",
    "\n",
    "# uncomment this if you want to use the GUI\n",
    "#cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client, show url we can go to to monitor progress\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scan data from bucket and make a simple list of file names\n",
    "\n",
    "While we recommend using 'import cloudcatalog' to fetch catalog lists, below we use the bare metal read from S3 to give a generalized example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our list of files/s3 objects\n",
    "import os\n",
    "import json\n",
    "if os.path.isfile('s3_data.json'):\n",
    "    with open ('s3_data.json') as f:\n",
    "        s3_files = json.load(f)['files']\n",
    "else:\n",
    "    # initialize connection to S3 bucket\n",
    "    s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    objs = s3_client.list_objects(Bucket=bucket_name, Prefix=bucket_path)\n",
    "    \"\"\" Iterates through all the objects, doing the pagination for you. Each obj\n",
    "     is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "     get to get the whole body.\n",
    "    \"\"\"\n",
    "    s3_files = []\n",
    "    for obj in objs['Contents']: #bucket.objects.all():\n",
    "        key = obj['Key']\n",
    "        if search ('fits', key):\n",
    "            s3_files.append('s3://'+bucket_name+'/'+key) \n",
    "    # write / cache files to local listing (speed purposes)\n",
    "    with open('s3_data.json', 'w') as outfile:\n",
    "        json.dump({'files' : s3_files}, outfile)\n",
    "    \n",
    "print(len(s3_files),\"files available, sample file:\",s3_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define some routines we will use for doing work with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits\n",
    "import s3fs\n",
    "\n",
    "def DO_SCIENCE(mydata):\n",
    "    # you can put better science here\n",
    "    iirad = mydata.mean()\n",
    "    return iirad\n",
    "\n",
    "# these are variable helpful handler functions\n",
    "def s3url_to_bucketkey(s3url: str): # -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts the S3 bucket name and file key from an S3 URL.\n",
    "    e.g. s3://mybucket/mykeypart1/mykeypart2/fname.fits -> mybucket, mykeypart1/mykeypart2/fname.fits\n",
    "    \"\"\"\n",
    "    name2 = re.sub(r\"s3://\",\"\",s3url)\n",
    "    s = name2.split(\"/\",1)\n",
    "    return s[0], s[1]\n",
    "\n",
    "def process_fits_s3(s3key:str): # -> Tuple[str, float]:\n",
    "    \"\"\" For a single FITS file, read it from S3, grab the header and\n",
    "        data, then do the DO_SCIENCE() call of choice\n",
    "    \"\"\"\n",
    "    sess = boto3.session.Session() # do this each open to avoid thread problem 'credential_provider'\n",
    "    s3c = sess.client(\"s3\")\n",
    "    mybucket,mykey = s3url_to_bucketkey(s3key)\n",
    "    try:\n",
    "        fobj = s3c.get_object(Bucket=mybucket,Key=mykey)\n",
    "        rawdata = fobj['Body'].read()\n",
    "        bdata = io.BytesIO(rawdata)\n",
    "        hdul = astropy.io.fits.open(bdata,memmap=False)        \n",
    "        date = hdul[1].header['T_OBS']\n",
    "        irrad = DO_SCIENCE(hdul[1].data)\n",
    "        print(date,irrad)\n",
    "    except:\n",
    "        print(\"Error fetching \",s3key)\n",
    "        date, irrad = None, None\n",
    "        \n",
    "    return date, irrad\n",
    "\n",
    "def work_on_data (client:dask.distributed.client.Client, bucket_name:str, files:list=[])->int:\n",
    "    \"\"\" \n",
    "    Main routine which Dask will use to 'do work'. Each worker will run this.\n",
    "    \"\"\"\n",
    "    # simple version step 1, do it\n",
    "    mean_irrad = client.map(process_fits_s3, s3_files)\n",
    "\n",
    "    # trigger distributed task, marshall result back to local memory\n",
    "    all_data = client.gather(mean_irrad)\n",
    "\n",
    "    # return the primary header back for analysis\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the cloud processing, using Dask to 'burst' into other VMs\n",
    "Using our gathered list of FITS files, chunk it out in batches and provide file list chunks to the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if n_files > len(s3_files):\n",
    "    n_files = len(s3_files)\n",
    "    \n",
    "def chunks(lst, n):\n",
    "    \"\"\" program to divide our file list into chunks for each worker \"\"\"\n",
    "    n = max(1, n)\n",
    "    return (lst[i:i+n] for i in range(0, len(lst), n))\n",
    "\n",
    "print (f\"workers: {n_workers}, cores/worker:{w_cores}, mem/worker: {w_memory}\")\n",
    "for files_to_process in chunks(s3_files[:n_files], batch_size):\n",
    "    returns = work_on_data(client, bucket_name, files_to_process)\n",
    "    print (f\"client:%s Finished %s files\" % (client,len(returns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "# Have Matplotlib create vector (svg) instead of raster (png) images\n",
    "#%config InlineBackend.figure_formats = ['svg'] \n",
    "#plt.figure()\n",
    "plt.plot_date(*zip(*returns))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
