{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "SMCE Heliophysics DaskHub S3 Bucket Tutorial\n",
    "</H1>\n",
    "</CENTER>\n",
    "<img src=\"./banner.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to process data in SMCE helio-public S3 bucket using Dask\n",
    "\n",
    "<i>Note:  This document is maintained through the SMCE HSD Cloud gitlab.</i>\n",
    "\n",
    "This is a simple example showing how to get a list of FITS files, run them through Dask workers to pull them from disk and examine the header keyword(s). \n",
    "\n",
    "You can use this notebook to test out various parameters you might feed to Dask; Consider the 'batch size', number of workers, number of cores per worker and memory per worker. Use the dashboard link to inspect how Dask is performing. Try both manual and automatic scaling strategies. See if you can get it to process 100 files in 20 sec or less!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First:  What is S3? \n",
    "\n",
    "S3 stands for \"Simple Storage Service,\" which provides object storage for for AWS.  https://aws.amazon.com/s3/ \n",
    "\n",
    "It allows people to query and access data from a common location reference.  The buckets can be made <a href=\"https://stackoverflow.com/q/16784052\">web accessible to users outside of daskhub</a> if web access is enabled.    \n",
    "\n",
    "S3 buckets are individual storage elements. \n",
    "\n",
    "## Accessing S3 buckets\n",
    "\n",
    "To <a href= \"https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/ls.html\">get a list of the S3 buckets</a> on the SMCE Daskhub, enter this at a terminal prompt : <br>\n",
    "`aws s3 ls`\n",
    "\n",
    "To view the contents of a specific bucket, reference it with s3:// <br>\n",
    "`aws s3 ls s3://helio-public/`\n",
    ">            PRE SDO/\n",
    ">            PRE SOHO/\n",
    "\n",
    "(Note: \"PRE\" stands for prefix, so SDO/ is an AWS prefix with name SDO.) \n",
    "<hr>\n",
    "\n",
    "The external reference for this bucket is https://helio-public.s3.us-east-1.amazonaws.com/\n",
    "\n",
    "## Basic commands using S3 buckets\n",
    "\n",
    "To create a new directory, just reference it:<br>\n",
    "`aws s3 ls s3://helio-public/yourname/yourdir`\n",
    "\n",
    "then you can copy to the bucket as if it was a unix folder:<br>\n",
    "`aws s3 cp yourfile s3://helio-public/yourname/yourdir`\n",
    "\n",
    "copying multiple files is a bit more intricate, you need to put the multiple files in a directory first:<br>\n",
    "`aws s3 cp sourcedir/ s3://helio-public/yourname/yourdir --recursive`\n",
    "\n",
    "if you need access to a bucket that has restricted access, you have to run aws-mfa first:<br>\n",
    "`~/aws-mfa default`\n",
    "\n",
    "where default is a profile. To see available profiles:<br>\n",
    "`cat ~/.aws/credentials`\n",
    "\n",
    "you may need to change it to have execute permission first:<br>\n",
    "`chmod 755 ~/aws-mfa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (The code below needs to be commented for instructional purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import dask\n",
    "import io\n",
    "import logging\n",
    "import s3fs\n",
    "\n",
    "from astropy.io import fits\n",
    "from dask.distributed import Client\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the bucket to upload to\n",
    "bucket_name = 'helio-public'\n",
    "\n",
    "# location in the bucket to use\n",
    "bucket_path = '/SDO/AIA/'\n",
    "\n",
    "# number of workers to use, for automatic scaling, our max number\n",
    "n_workers = 10\n",
    "\n",
    "# memory per worker (in Gb)\n",
    "w_memory = 2\n",
    "\n",
    "# cores per worker\n",
    "w_cores = 2\n",
    "\n",
    "# number of files to test against\n",
    "n_files = 50\n",
    "\n",
    "# Number of files we release to be worked on by all workers at a time\n",
    "# the higher the number the more files being processed concurrently, but also\n",
    "# the greater the memory consumed. \n",
    "batch_size = 100\n",
    "\n",
    "# use Manual (if False, then uses Automatic scaling)\n",
    "use_manual_scaling = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the cluster and assign the client to the cluster, display the cluster widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "from dask_gateway import Gateway, GatewayCluster\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "\n",
    "# We're setting some defaults here just for grins... \n",
    "# I like the pangeo/base-notebook image for the workers since it has almost every library you'd need on a worker\n",
    "# In our environment, without setting these, the widget will default to the same image that the notebook itself is running, \n",
    "# as well as 2 cores and 4GB memory per worker\n",
    "\n",
    "options.worker_cores=w_cores\n",
    "options.worker_memory=w_memory\n",
    "# options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cluster \u001b[38;5;241m=\u001b[39m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mget_client()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_manual_scaling:\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# manual scaling (n_workers defined above)\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:639\u001b[0m, in \u001b[0;36mGateway.new_cluster\u001b[0;34m(self, cluster_options, shutdown_on_close, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_cluster\u001b[39m(\u001b[38;5;28mself\u001b[39m, cluster_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shutdown_on_close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;124;03m\"\"\"Submit a new cluster to the gateway, and wait for it to be started.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    Same as calling ``submit`` and ``connect`` in one go.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    cluster : GatewayCluster\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGatewayCluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublic_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_public_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshutdown_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshutdown_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:814\u001b[0m, in \u001b[0;36mGatewayCluster.__init__\u001b[0;34m(self, address, proxy_address, public_address, auth, cluster_options, shutdown_on_close, asynchronous, loop, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    804\u001b[0m     address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    813\u001b[0m ):\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublic_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpublic_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcluster_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshutdown_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshutdown_on_close\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:919\u001b[0m, in \u001b[0;36mGatewayCluster._init_internal\u001b[0;34m(self, address, proxy_address, public_address, auth, cluster_options, cluster_kwargs, shutdown_on_close, asynchronous, loop, name)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masynchronous:\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_internal\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:343\u001b[0m, in \u001b[0;36mGateway.sync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(\n\u001b[1;32m    340\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop\u001b[38;5;241m.\u001b[39masyncio_loop\n\u001b[1;32m    341\u001b[0m )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     future\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client()\n",
    "\n",
    "if use_manual_scaling:\n",
    "    \n",
    "    # manual scaling (n_workers defined above)\n",
    "    cluster.scale(n_workers)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Adaptively scale between 1 and n_workers (the max)\n",
    "    cluster.adapt(minimum=1, maximum=n_workers)\n",
    "\n",
    "# uncomment this if you want to use the GUI\n",
    "#cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client, show url we can go to to monitor progress\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scan data from bucket and make a simple list of file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our list of files/s3 objects\n",
    "cached = False\n",
    "import json\n",
    "if (cached):\n",
    "    with open ('s3_data.json') as f:\n",
    "        s3_files = json.load(f)['files']\n",
    "\n",
    "else:\n",
    "\n",
    "    # initialize connection to S3 bucket\n",
    "    s3_client = boto3.resource('s3')\n",
    "    bucket = s3_client.Bucket(bucket_name)\n",
    "\n",
    "    # Iterates through all the objects, doing the pagination for you. Each obj\n",
    "    # is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "    # get to get the whole body.\n",
    "    s3_files = []\n",
    "    for obj in bucket.objects.all():\n",
    "        key = obj.key\n",
    "\n",
    "        if search ('fits', key):\n",
    "            s3_files.append(obj.key)\n",
    "    \n",
    "    # write / cache files to local listing (speed purposes)\n",
    "\n",
    "    with open('s3_data.json', 'w') as outfile:\n",
    "        json.dump({'files' : s3_files}, outfile)\n",
    "    \n",
    "\n",
    "s3_files[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define some routines we will use for doing work with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_fits_s3 (s3_file_name:str, bucket_name:str)->object:\n",
    "    \n",
    "    \"\"\" Open a FITS file on an S3 bucket, pass back a binary blob of file info.\n",
    "    \"\"\"\n",
    "     \n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    with fs.open(bucket_name+'/'+s3_file_name, 'rb') as f:\n",
    "        #fits_hdul = fits.open(io.BytesIO(f.read()))\n",
    "        # fits_hdul.info()\n",
    "    \n",
    "        # return bytes\n",
    "        return f.read()\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_header(f_bytes:object)->str:\n",
    "    \n",
    "    \"\"\" Marshal the binary blob into a astropy FITS object, then read the header and pass back \n",
    "    \"\"\"\n",
    "    try: \n",
    "        hdul = fits.open(io.BytesIO(f_bytes))\n",
    "        return hdul[0].header\n",
    "    except Exception as ex:\n",
    "        # bad file? Some of these are missing the END keyword!!\n",
    "        # for now, lets simply side-step it and move on\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "    \n",
    "def work_on_data (client:dask.distributed.client.Client, bucket_name:str, files:list=[])->int:\n",
    "    \n",
    "    \"\"\" \n",
    "    Main routine which Dask will use to 'do work'. Each worker will run this.\n",
    "    \"\"\"\n",
    "    \n",
    "    # pull files from S3 as bytes\n",
    "    f_bytes_list = client.map(open_fits_s3, files, bucket_name=bucket_name)\n",
    "    \n",
    "    # read bytes as FITS file, grab header\n",
    "    hduls = client.map(get_header, f_bytes_list)\n",
    "    \n",
    "    # trigger distributed task, marshall result back to local memory\n",
    "    try:\n",
    "        headers = client.gather(hduls)\n",
    "    except Error as er:\n",
    "        print (f\"Caught exception: {er}\")\n",
    "    \n",
    "    # return a result of some kind, lets parse gathered headers to return value of the \n",
    "    # AIL1FILE keyword\n",
    "    return [hdr['AIL1FILE'] for hdr in headers if hdr != None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Do the cloud processing, using Dask to 'burst' into other VMs\n",
    "Using our gathered list of FITS files, chunk it out in batches and provide file list chunks to the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def chunks(lst, n):\n",
    "    \"\"\" program to divide our file list into chunks for each worker \"\"\"\n",
    "    n = max(1, n)\n",
    "    return (lst[i:i+n] for i in range(0, len(lst), n))\n",
    "\n",
    "print (f\"workers: {n_workers}, cores/worker:{w_cores}, mem/worker: {w_memory}\")\n",
    "\n",
    "for files_to_process in chunks(s3_files[:n_files], batch_size):\n",
    "\n",
    "    r = work_on_data(client, bucket_name, files_to_process)\n",
    "    print (f\"client:%s Finished %s files\" % (client,len(r)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
