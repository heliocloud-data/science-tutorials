{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edac1c14-2f71-4434-a3d3-ef4cfefddd6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Notebook\n",
    "\n",
    "This notebook is intended for testing, to ensure HelioCloud capabilities are all in place. The items it tests are:\n",
    "\n",
    "1) Ability to import core PyHC packages (container test)\n",
    "2) Ability to access HelioCloud data listings via cloudcatalog\n",
    "3) Ability to read FITS, CDF and NetCDF files from S3 storage (S3 test, container test)\n",
    "4) Ability to run a task with Lambdas (lambdas test)\n",
    "5) Ability to plot data using matplotlib (plotting test)\n",
    "6) Ability to save data locally, then read it back (permissions test)\n",
    "7) Ability to run very basic functions from PyHC core packages HAPI, SunPy, Kamodo, PlasmaPy, pySat, PySpedas, and SpacePy\n",
    "8) Ability to spin up a Dask cluster to run a task (dask test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92ddc5-b67f-419b-bce9-7e1192859132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Container test\n",
    "import cloudcatalog\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.io.fits\n",
    "import astropy.units as u\n",
    "import cdflib\n",
    "import xarray as xr\n",
    "# S3\n",
    "import boto3\n",
    "import s3fs\n",
    "# utilities\n",
    "import io\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "# PyHC core as of 2023\n",
    "from hapiclient import hapi\n",
    "from hapiplot import hapiplot\n",
    "import sunpy.map\n",
    "import sunpy.data.sample  \n",
    "import spacepy.toolbox as tb\n",
    "from plasmapy.formulary import beta\n",
    "import pysat\n",
    "import pyspedas\n",
    "from pytplot import tplot, get_data\n",
    "from kamodo import Kamodo\n",
    "# Dask\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway, GatewayCluster\n",
    "\n",
    "imports_loaded_flag = True\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195fa3c4-2578-43a5-aab2-22da5e9761ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optional, prettier inline Notebook plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afebbf7-65cc-4aa7-b0a4-be911f5e43c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HelioCloud shared cloud file registry (cloudcatalog)\n",
    "\n",
    "This is a simple standard for any dataset that enables users to access it via an API or directly.  The short definition is:\n",
    "    * S3 disks have a 'catalog.json' describing their datasets\n",
    "    * Each dataset has a <dataid>_YYYY.csv index file of its contents\n",
    "    * These indexes have the form \"time, s3_location, filesize\" (plus optional metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d17d6-8b02-400a-b0c3-48482012c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import cloudcatalog\n",
    "    debug = True\n",
    "cr=cloudcatalog.CatalogRegistry()\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "frID = \"aia_0094\"\n",
    "myjson = fr.get_entry(frID)\n",
    "start, stop = myjson['start'], myjson['stop']\n",
    "file_registry1 = fr.request_cloud_catalog(frID, start_date=start, stop_date=stop, overwrite=False)\n",
    "# And convert that richer data to a list of files to process\n",
    "filelist = file_registry1['datakey'].to_list()\n",
    "if debug:\n",
    "    print(len(filelist))\n",
    "assert len(filelist) > 1400000\n",
    "\n",
    "# test read\n",
    "fs=s3fs.S3FileSystem(anon=False)\n",
    "fgrab = fs.open(filelist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874579ad-312c-44d0-8291-4764fc68bf8e",
   "metadata": {},
   "source": [
    "## FITS, CDF and NetCDF reading off S3\n",
    "\n",
    "We now read one FITS file from the above list and print a simple sum of it to show we read it properly.  We use the 's3fs' protocol to bring it into AstroPy's FITS reader.  Some versions of astropy already can read S3 files directly.  Here we use \"import astropy.fits\" and \"import s3fs\" for FITS; \"import cdflib\" for CDF; and \"import xarray as xr\" and \"import s3fs\" for NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ac85f-7742-4839-b3e1-7c62001392fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import astropy.io.fits\n",
    "    import s3fs\n",
    "    debug = True\n",
    "s3name = \"s3://gov-nasa-hdrl-data1/demo-data/sdo_aia.fits\"\n",
    "try:\n",
    "    hdu = astropy.io.fits.open(s3name)\n",
    "except:\n",
    "    print(\"astropy not able to read S3 support, re-trying\")\n",
    "    fs=s3fs.S3FileSystem(anon=True)\n",
    "    fgrab = fs.open(s3name)\n",
    "    hdu = astropy.io.fits.open(fgrab)\n",
    "if debug:\n",
    "    print(hdu[0].header[0:10])\n",
    "    hdu.info()\n",
    "    print(sum(sum(hdu[1].data)))\n",
    "assert sum(sum(hdu[1].data)) == 28622080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c49677-4088-4659-823e-79f965adcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF reading from S3 cloud\n",
    "if 'imports_loaded_flag' not in locals():\n",
    "    import cdflib\n",
    "    import math\n",
    "    debug = True\n",
    "s3name=\"s3://gov-nasa-hdrl-data1/demo-data/mms_fgm.cdf\"\n",
    "with cdflib.CDF(s3name) as cdfin1:\n",
    "    assert math.isclose(sum(sum(cdfin1['mms1_fgm_b_gse_brst_l2'])), 415662.265625)\n",
    "    if debug:\n",
    "        print(cdfin1.cdf_info())\n",
    "\"\"\"\n",
    "Can also do CDF reading in a URL, noted here but not run\n",
    "s3name=\"https://gov-nasa-hdrl-data1.s3.amazonaws.com/demo-data/mms_fgm.cdf\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b97e62-2d8d-474f-a698-b7776f49b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NetCDF via xarray, using s3fs, reading from S3 cloud\n",
    "if 'imports_loaded_flag' not in locals():\n",
    "    import s3fs\n",
    "    import xarray as xr\n",
    "    debug = True\n",
    "s3name=\"s3://gov-nasa-hdrl-data1/demo-data/guvi_spect.nc\"\n",
    "fs=s3fs.S3FileSystem(anon=True)\n",
    "fgrab = fs.open(s3name)\n",
    "dataset = xr.open_dataset(fgrab)\n",
    "if debug:\n",
    "    print(f\"Dimensions = {dataset.dims}\")\n",
    "assert math.isclose(sum(dataset['DISKCOUNTSDATA_DAY'].data[:,0]),4384917.771728516)\n",
    "dataset.close()\n",
    "fgrab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424825d-4caf-4d4b-9403-e07cdb3e3716",
   "metadata": {},
   "source": [
    "## Ability to run a task with Lambdas (lambdas test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19079fc8-9c3e-4166-9c38-13d24bd46f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import cloudcatalog\n",
    "cr = cloudcatalog.CatalogRegistry()\n",
    "fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\",cache=False)\n",
    "dataset_id1 = 'mms1_feeps_brst_electron'\n",
    "start = '2020-02-01T00:00:00Z'\n",
    "stop =   '2020-02-02T00:00:01Z'\n",
    "file_registry1 = fr.request_cloud_catalog(dataset_id1, start_date=start, stop_date=stop, overwrite=False)\n",
    "print(f\"{len(file_registry1)} files, operating lambda on first ten.\")\n",
    "print('Python Hash of File | Start Date | File Size')\n",
    "fr.stream(file_registry1[0:10], lambda bo, d, e, f: print(hash(bo.read()), d.replace(' ', 'T')+'Z', e, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec6714-4031-46b9-8701-56bb52e13b9a",
   "metadata": {},
   "source": [
    "## Ability to plot data using matplotlib (plotting test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72015f9-1866-42f2-b6b0-fd4955d9e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import cdflib\n",
    "    import matplotlib.pyplot as plt\n",
    "s3key = 's3://gov-nasa-hdrl-data1/demo-data/mms1_feeps_brst_ele.cdf'\n",
    "cdf = cdflib.CDF(s3key)\n",
    "var_data = cdf.varget(1)\n",
    "plt.figure()\n",
    "plt.plot(var_data)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0f138-ecf3-4110-b2a4-93e129b43188",
   "metadata": {},
   "source": [
    "# Ability to save data locally, then read it back (permissions test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b899a9-38a4-496d-aafb-cf2b6c4d6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import pickle\n",
    "sample_data = [0, 3, 9]\n",
    "with open('test.pickle','wb') as fout:\n",
    "    pickle.dump(sample_data, fout)\n",
    "with open('test.pickle','rb') as fin:\n",
    "    new_data = pickle.load(fin)\n",
    "assert new_data == sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2929ac-3131-48aa-9ecf-bcb9973ae162",
   "metadata": {},
   "source": [
    "## PyHC package tests\n",
    "\n",
    "Tests HAPI, SunPy, Kamodo, PlasmaPy, pySat, PySpedas, and SpacePy using very brief excerpts of the examples from either their website tutorials or from the 2022 PyHC Summer School."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0cdba3-0470-4cb6-8115-c32a8d9fa195",
   "metadata": {},
   "source": [
    "### HAPI\n",
    "Test from 2022 PyHC Summer School. Requires \"from hapiclient import hapi; from hapiplot import hapiplot\", also \"import math\" for the assertion math.isclose() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dce13-2246-4686-9d1c-9d8a04593f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    from hapiclient import hapi\n",
    "    from hapiplot import hapiplot\n",
    "    import math\n",
    "# HAPI test, OMNIWeb data\n",
    "# The data server\n",
    "server     = 'https://cdaweb.gsfc.nasa.gov/hapi'\n",
    "# The data set\n",
    "dataset    = 'OMNI2_H0_MRG1HR'\n",
    "# Start and stop times\n",
    "start      = '2021-10-25T00:00:00Z'\n",
    "stop       = '2021-12-01T00:00:00Z'\n",
    "# The HAPI convention is that parameters is a comma-separated list. Here we request two parameters.\n",
    "parameters = 'DST1800,Proton_QI1800'\n",
    "# Configuration options for the hapi function.\n",
    "opts = {'logging': True, 'usecache': False, 'cachedir': './hapicache' }\n",
    "# Get parameter data. See section 5 for for information on getting available datasets and parameters\n",
    "data, meta = hapi(server, dataset, parameters, start, stop, **opts)\n",
    "hapiplot(data, meta)\n",
    "assert meta['x_time.min'] == '2021-10-25T00:00:00Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dda4cb-3d60-42c4-b316-ed1031023bdc",
   "metadata": {},
   "source": [
    "### SunPy\n",
    "Test from https://docs.sunpy.org/en/stable/tutorial/maps.html.  Requires \"import sunpy.map\" and \"import sunpy.data.sample\", also \"import math\" for the assertion math.isclose() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101decc3-3680-49b2-abe8-cb63db2d08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import sunpy.map\n",
    "    import sunpy.data.sample\n",
    "    import astropy.units as u\n",
    "    import matplotlib.pyplot as plt\n",
    "sunpy.data.sample.AIA_171_IMAGE\n",
    "my_map = sunpy.map.Map(sunpy.data.sample.AIA_171_IMAGE)  \n",
    "my_map.quicklook()  # is quicklook doing anything? I do not see a plot\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=my_map)\n",
    "my_map.plot(axes=ax, clip_interval=(1, 99.5)*u.percent)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "print(\"Mean of image:\",my_map.data.mean())\n",
    "assert math.isclose(my_map.data.mean(),427.02252,rel_tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b2127-5803-4f0b-a4ed-7ea883bbe33b",
   "metadata": {},
   "source": [
    "### Kamodo\n",
    "From 2022 PyHC Summer School, notebooks 03 and 04. Requires \"from kamodo import Kamodo\" and, for making an example dataset, \"import numpy as np\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576d790-44f2-4324-87b3-51e2a8ae84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    from kamodo import Kamodo\n",
    "    import numpy as np\n",
    "x, y, z = np.meshgrid(np.linspace(-2,2,4),\n",
    "                      np.linspace(-3,3,6),\n",
    "                      np.linspace(-5,5,10))\n",
    "points = np.array(list(zip(x.ravel(), y.ravel(), z.ravel())))\n",
    "def fvec_Ncomma3(rvec_Ncomma3 = points):\n",
    "    return rvec_Ncomma3\n",
    "k = Kamodo(fvec_Ncomma3 = fvec_Ncomma3)\n",
    "k.plot('fvec_Ncomma3')\n",
    "#\n",
    "k_test = Kamodo('f(x[cm])[kg/m^3]=x^2-x-1')\n",
    "assert k_test.f(3) == 3**2 - 3 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b310f8-69a1-4826-b81f-b6d4e9b0ef81",
   "metadata": {},
   "source": [
    "### SpacePy\n",
    "SpacePy Toolbox example from their quickstart guide, https://spacepy.github.io/quickstart.html.  Requires \"import spacepy.toolbox as tb\", also \"import numpy as np\" for generating the same dataset, also \"import math\" for the assertion math.isclose() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28228483-56b3-4f88-a934-86b49423d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import spacepy.toolbox as tb\n",
    "    import numpy as np\n",
    "    import math\n",
    "a = {'entry1':'val1', 'entry2':2, 'recurse1':{'one':1, 'two':2}}\n",
    "tb.dictree(a)\n",
    "dat = np.arange(100)\n",
    "bh = tb.binHisto(dat)\n",
    "print(bh)\n",
    "assert math.isclose(bh[0], 21.328903431315652)\n",
    "assert bh[1] == 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933906f-db50-4cda-b8dd-65ec1df301e0",
   "metadata": {},
   "source": [
    "## PlasmaPy\n",
    "Example from 2022 PyHC summer school. Requires \"from plasmapy.formulary import beta\"(alt: \"from plasmapy.formulary import \\*\"), also \"import astropy.units as u\" for setup and \"import math\" for the assertion math.isclose() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85afaf9-9810-4bcf-a607-0919d93fe6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    from plasmapy.formulary import beta\n",
    "    import astropy.units as u\n",
    "    import math\n",
    "# Let's start by defining some plasma parameters for an active region in the solar corona.\n",
    "# When we use these parameters in beta, we find that Î² is quite small so that the corona is magnetically dominated.\n",
    "B_corona = 50 * u.G\n",
    "n_corona = 1e9 * u.cm ** -3\n",
    "T_corona = 1 * u.MK\n",
    "b=beta(T_corona, n_corona, B_corona)\n",
    "assert math.isclose(b, 0.0013879797625431325, rel_tol=1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db050629-da63-4d69-8475-b21633db7ed2",
   "metadata": {},
   "source": [
    "### pySat\n",
    "Example taken from their quickstart guide, https://pysat.readthedocs.io/en/latest/quickstart.html. Requires \"import pysat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7786b45-2954-4291-8fb0-d2d09e3acee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import pysat\n",
    "    pysat.params['data_dirs'] = \".\"\n",
    "# Testing out the xarray installation\n",
    "inst = pysat.Instrument('pysat', 'testmodel')\n",
    "inst.load(2009, 1)\n",
    "assert sum(sum(sum(inst.data['dummy1'].values))) >= 945378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8d52f-9dae-48a0-8054-b6139db86285",
   "metadata": {},
   "source": [
    "### PySpedas\n",
    "Requires \"import pyspedas\" and \"from pytplot import tplot, get_data\", also \"import math\" for the assertion math.isclose() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00643e85-5dd6-4540-832c-42b63206e78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import pyspedas\n",
    "    from pytplot import tplot, get_data\n",
    "    import math\n",
    "time_range = ['2020-04-20/06:00', '2020-04-20/08:00']\n",
    "pyspedas.solo.mag(trange=time_range, time_clip=True)\n",
    "pyspedas.mms.fgm(trange=time_range, time_clip=True, probe=2,always_prompt=False)\n",
    "tplot(['B_RTN','mms2_fgm_b_gsm_srvy_l2_bvec'])\n",
    "mag_data = get_data('mms2_fgm_b_gse_srvy_l2_bvec')\n",
    "assert math.isclose(sum(sum(mag_data.y)),152229.33203125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9feddd-ff01-4b93-a8b8-3029f8cd1b64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dask cluster 'burst' test\n",
    "\n",
    "We now use 4 cells to run a Dask cluster 'burst' test, sending 1000 files out to multiple worker nodes using Dask.  This can take up to 10 minutes to spin up the Cluster, but then this and subsequent jobs are very rapid.\n",
    "Cell 1 sets up the data environment. Cell 2 starts the cluster.  Cell 3 does the dask run (and can be re-run).  Cell 4 shuts down the cluster for good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd2fa4-23ed-4d22-96ba-efcda3d959e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'imports_loaded_flag' not in locals():\n",
    "    import cloudcatalog\n",
    "    import boto3\n",
    "    import dask\n",
    "    import io\n",
    "    import time\n",
    "    import re\n",
    "    import astropy.io.fits\n",
    "    from dask.distributed import Client\n",
    "    from dask_gateway import Gateway, GatewayCluster\n",
    "    \n",
    "def DO_SCIENCE(mydata):\n",
    "    # you can put better science here\n",
    "    iirad = mydata.mean()\n",
    "    return iirad\n",
    "\n",
    "def process_fits_s3(s3key:str): # -> Tuple[str, float]:\n",
    "    \"\"\" grabs an S3 file then runs DO_SCIENCE() on it \"\"\"\n",
    "    sess = boto3.session.Session() # do this each open to avoid thread problem 'credential_provider'\n",
    "    s3c = sess.client(\"s3\")\n",
    "    [mybucket,mykey] = re.sub(r\"s3://\",\"\",s3key).split(\"/\",1)\n",
    "    try:\n",
    "        fobj = s3c.get_object(Bucket=mybucket,Key=mykey)\n",
    "        rawdata = fobj['Body'].read()\n",
    "        bdata = io.BytesIO(rawdata)\n",
    "        hdul = astropy.io.fits.open(bdata,memmap=False)        \n",
    "        date = hdul[1].header['T_OBS']\n",
    "        irrad = DO_SCIENCE(hdul[1].data)\n",
    "    except:\n",
    "        print(\"Error fetching \",s3key)\n",
    "        date, irrad = None, None       \n",
    "    return date, irrad\n",
    "\n",
    "if 'file_registry1' not in locals():\n",
    "    fr=cloudcatalog.CloudCatalog(\"s3://gov-nasa-hdrl-data1/\")\n",
    "    frID = \"aia_0094\"\n",
    "    myjson = fr.get_entry(frID)\n",
    "    start, stop = myjson['start'], myjson['stop']\n",
    "    file_registry1 = fr.request_cloud_catalog(frID, start_date=start, stop_date=stop, overwrite=False)\n",
    "filelist = file_registry1['datakey'].to_list()\n",
    "s3_files = filelist[0:1000] # small test set to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d0955-f396-480c-b047-ec68c89dd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now we initialize the Dask gateway and cluster, using your above parameters, to set up the virtual machines that will subsequently operate on the data. We use some non-optimized Dask parameters for setting up the run.\"\"\"\n",
    "# from the daskhub tutorial, setting up dask\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.worker_cores = 4\n",
    "options.worker_memory = 2\n",
    "# initialize cluster and create client, takes < 15 seconds\n",
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client() # can also use 'client=Client(cluster)'\n",
    "cluster.adapt(minimum=1, maximum=4)\n",
    "# if useGUI: cluster\n",
    "# client # let us take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2efc6-7a81-4866-8596-b69130b4cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "now=time.time()\n",
    "time_irrad = client.map(process_fits_s3, s3_files) # do it\n",
    "all_data = client.gather(time_irrad) # gather results\n",
    "print(\"Done! Completed in time \",(time.time()-now)/60.0,\"minutes, on\",len(all_data),\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937827c9-bb59-496d-8684-f1b1b31f9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# always shutdown once all cluster uses and re-uses are over\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87784be3-55a9-4115-b1c2-6375b4c293d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done all tests\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
